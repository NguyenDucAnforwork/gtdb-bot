# src/memory/memory_manager.py
import os
from typing import List, Dict, Any, Optional
from mem0 import Memory
from dotenv import load_dotenv

load_dotenv()


class MemoryManager:
    """Manages conversation memory using Mem0 with Qdrant Cloud."""
    
    def __init__(self):
        """Initialize Memory with Qdrant Cloud configuration."""
        QDRANT_URL = os.getenv("QDRANT_URL")
        QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        
        config = {
            "llm": {
                "provider": "openai",
                "config": {
                    "model": "gpt-4o-mini",
                    "temperature": 0,
                    "api_key": OPENAI_API_KEY
                }
            },
            "embedder": {
                "provider": "openai",
                "config": {
                    "model": "text-embedding-3-small",
                    "api_key": OPENAI_API_KEY
                }
            },
            "vector_store": {
                "provider": "qdrant",
                "config": {
                    "collection_name": "mem0_chatbot",
                    "url": QDRANT_URL,
                    "api_key": QDRANT_API_KEY,
                }
            },
            "version": "v1.1"
        }
        
        print("ðŸ§  Initializing MemoryManager with Qdrant Cloud...")
        self.memory = Memory.from_config(config)
    
    def get_memories(self, user_id: str, limit: int = 15) -> List[str]:
        """
        Get all memories for a user.
        
        Args:
            user_id: User identifier
            limit: Maximum number of memories to retrieve
            
        Returns:
            List of memory strings
        """
        try:
            res = self.memory.get_all(user_id=user_id, limit=limit)
            all_memories = res.get('results', [])
            
            # Extract just the 'memory' text field
            memories = [mem['memory'] for mem in all_memories]
            
            print(f"ðŸ§  Retrieved {len(memories)} memories for user {user_id}")
            return memories
            
        except Exception as e:
            print(f"âš ï¸ Error retrieving memories: {e}")
            return []
    
    def get_recent_context(self, user_id: str, n: int = 15) -> str:
        """
        Get recent conversation context formatted for LLM.
        
        Args:
            user_id: User identifier
            n: Number of recent memories to include
            
        Returns:
            Formatted context string
        """
        memories = self.get_memories(user_id, limit=n)
        
        if not memories:
            return ""
        
        # Use last n memories as recent context
        recent_memories = memories[-n:] if len(memories) > n else memories
        context_str = "\n".join([f"- {mem}" for mem in recent_memories])
        
        return context_str
    
    def save_conversation(
        self, 
        user_id: str, 
        query: str, 
        response: str,
        should_save_fn: Optional[callable] = None
    ) -> bool:
        """
        Save conversation to memory with optional validation.
        
        Args:
            user_id: User identifier
            query: User query
            response: Bot response
            should_save_fn: Optional function to validate if should save
            
        Returns:
            True if saved, False otherwise
        """
        # Check if should save using provided function
        if should_save_fn and not should_save_fn(query, response):
            print("âŒ Not saved to memory (validation failed)")
            return False
        
        try:
            messages_to_save = [
                {"role": "user", "content": query},
                {"role": "assistant", "content": response}
            ]
            
            # Use infer=False to store raw conversation
            self.memory.add(messages_to_save, user_id=user_id, infer=False)
            
            print(f"ðŸ’¾ Saved to Mem0 memory:")
            print(f"   User: {query[:80]}...")
            print(f"   Bot: {response[:80]}...")
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ Error saving to memory: {e}")
            return False
    
    def clear_user_memories(self, user_id: str) -> bool:
        """
        Clear all memories for a user.
        
        Args:
            user_id: User identifier
            
        Returns:
            True if successful, False otherwise
        """
        try:
            self.memory.delete_all(user_id=user_id)
            print(f"ðŸ§¹ Cleared all memories for user {user_id}")
            return True
        except Exception as e:
            print(f"âš ï¸ Error clearing memories: {e}")
            return False
    
    def add_conversation(self, query: str, response: str, user_id: str) -> bool:
        """
        Add a conversation to memory.
        
        Args:
            query: User query
            response: Bot response
            user_id: User identifier
            
        Returns:
            True if saved successfully
        """
        try:
            messages_to_save = [
                {"role": "user", "content": query},
                {"role": "assistant", "content": response}
            ]
            
            # Critical: Use infer=False to store raw conversation
            self.memory.add(messages_to_save, user_id=user_id, infer=False)
            
            print(f"ðŸ’¾ Saved to memory for user {user_id}")
            return True
            
        except Exception as e:
            print(f"âš ï¸ Error adding conversation: {e}")
            return False
    
    def get_context(self, user_id: str, limit: int = 15) -> str:
        """
        Get formatted conversation context for a user.
        
        Args:
            user_id: User identifier
            limit: Number of recent memories to retrieve
            
        Returns:
            Formatted context string
        """
        return self.get_recent_context(user_id, n=limit)
    
    def validate_memory_sufficiency(self, memory_context: str, question: str) -> bool:
        """
        Use LLM to validate if memory context is sufficient to answer the question.
        
        Args:
            memory_context: Formatted memory context
            question: User question
            
        Returns:
            True if memory has sufficient information
        """
        if not memory_context or not memory_context.strip():
            return False
        
        try:
            from src.generation.openai_generator import get_llm
            from langchain_core.prompts import ChatPromptTemplate
            from langchain_core.output_parsers import StrOutputParser
            
            llm = get_llm()
            validator_prompt = ChatPromptTemplate.from_template(
                """Báº¡n lÃ  má»™t chuyÃªn gia Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i.

NGá»® Cáº¢NH Tá»ª Bá»˜ NHá»š CUá»˜C TRÃ’ CHUYá»†N:
{memory_context}

CÃ‚U Há»ŽI Cá»¦A NGÆ¯á»œI DÃ™NG: {question}

NHIá»†M Vá»¤: ÄÃ¡nh giÃ¡ xem ngá»¯ cáº£nh trÃªn cÃ³ Äá»¦ THÃ”NG TIN Ä‘á»ƒ tráº£ lá»i Cá»¤ THá»‚ vÃ  CHÃNH XÃC cÃ¢u há»i khÃ´ng?

TIÃŠU CHÃ ÄÃNH GIÃ:
âœ… CÃ“ Äá»¦ THÃ”NG TIN náº¿u:
- Ngá»¯ cáº£nh chá»©a TRá»°C TIáº¾P cÃ¢u tráº£ lá»i vá»›i Sá» LIá»†U Cá»¤ THá»‚
- CÃ¢u há»i lÃ  "tá»•ng cá»™ng/cá»™ng láº¡i" vÃ  ngá»¯ cáº£nh cÃ³ Äáº¦Y Äá»¦ cÃ¡c con sá»‘ cáº§n tÃ­nh
- CÃ¢u há»i há»i láº¡i vá» Ä‘iá»u Ä‘Ã£ Ä‘Æ°á»£c nÃ³i trÆ°á»›c Ä‘Ã³ trong ngá»¯ cáº£nh

âŒ KHÃ”NG Äá»¦ THÃ”NG TIN náº¿u:
- Ngá»¯ cáº£nh chá»‰ cÃ³ cÃ¢u há»i, KHÃ”NG cÃ³ cÃ¢u tráº£ lá»i
- Ngá»¯ cáº£nh tráº£ lá»i MÆ  Há»’, khÃ´ng cÃ³ sá»‘ liá»‡u cá»¥ thá»ƒ
- CÃ¢u há»i yÃªu cáº§u thÃ´ng tin Má»šI hoÃ n toÃ n khÃ´ng cÃ³ trong ngá»¯ cáº£nh
- Ngá»¯ cáº£nh cÃ³ cÃ¢u tráº£ lá»i nhÆ°ng KHÃ”NG LIÃŠN QUAN Ä‘áº¿n cÃ¢u há»i hiá»‡n táº¡i

CHá»ˆ TRáº¢ Lá»œI Báº°NG Má»˜T Tá»ª: "YES" (cÃ³ Ä‘á»§) hoáº·c "NO" (khÃ´ng Ä‘á»§)"""
            )
            
            chain = validator_prompt | llm | StrOutputParser()
            result = chain.invoke({
                "memory_context": memory_context,
                "question": question
            }).strip().upper()
            
            return result == "YES"
            
        except Exception as e:
            print(f"âš ï¸ Memory validation error: {e}")
            return False
    
    def should_save_memory(self, query: str, response: str) -> bool:
        """
        Use LLM to decide whether to save conversation to memory.
        
        Args:
            query: User query
            response: Bot response
            
        Returns:
            True if conversation should be saved
        """
        try:
            from src.generation.openai_generator import get_llm
            from langchain_core.prompts import ChatPromptTemplate
            from langchain_core.output_parsers import StrOutputParser
            
            llm = get_llm()
            memory_prompt = ChatPromptTemplate.from_template(
                """PhÃ¢n tÃ­ch cuá»™c trÃ² chuyá»‡n vÃ  quyáº¿t Ä‘á»‹nh cÃ³ nÃªn lÆ°u vÃ o bá»™ nhá»› dÃ i háº¡n khÃ´ng?
            
CÃ¢u há»i: "{query}"
Tráº£ lá»i: "{response}"
            
Tráº£ lá»i "YES" náº¿u:
- Chá»©a thÃ´ng tin phÃ¡p lÃ½ quan trá»ng (má»©c pháº¡t, quy Ä‘á»‹nh, Ä‘iá»u luáº­t cá»¥ thá»ƒ)
- Chá»©a thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng (tÃªn, nghá» nghiá»‡p, Ä‘á»‹a Ä‘iá»ƒm)
- LÃ  cuá»™c trÃ² chuyá»‡n cÃ³ giÃ¡ trá»‹ tham kháº£o lÃ¢u dÃ i
- NgÆ°á»i dÃ¹ng Ä‘ang há»i vá» má»™t chá»§ Ä‘á» cá»¥ thá»ƒ cáº§n nhá»›
            
Tráº£ lá»i "NO" náº¿u:
- Chá»‰ lÃ  chÃ o há»i Ä‘Æ¡n giáº£n ("xin chÃ o", "cáº£m Æ¡n", "hi", "hello")
- Tráº£ lá»i mÆ¡ há»“, khÃ´ng cá»¥ thá»ƒ, khÃ´ng cÃ³ sá»‘ liá»‡u
- KhÃ´ng cÃ³ giÃ¡ trá»‹ tham kháº£o
            
Chá»‰ tráº£ lá»i "YES" hoáº·c "NO"."""
            )
            
            chain = memory_prompt | llm | StrOutputParser()
            result = chain.invoke({"query": query, "response": response}).strip().upper()
            return result == "YES"
            
        except Exception as e:
            print(f"âš ï¸ Memory save decision error: {e}")
            # Fallback to simple heuristic
            legal_keywords = ['pháº¡t', 'luáº­t', 'quy Ä‘á»‹nh', 'nghá»‹ Ä‘á»‹nh', 'tÃ´i tÃªn', 'tÃ´i lÃ ', 'tÃ´i Ä‘ang']
            return any(word in query.lower() for word in legal_keywords) and len(response) > 50
    
    def format_context(self, memories: List[str]) -> str:
        """
        Format a list of memories into a context string.
        
        Args:
            memories: List of memory strings
            
        Returns:
            Formatted context string
        """
        if not memories:
            return ""
        return "\n".join([f"- {mem}" for mem in memories])
