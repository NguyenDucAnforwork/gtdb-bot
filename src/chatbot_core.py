# src/chatbot_core.py
from typing import Dict, Any, List, Optional
from enum import Enum
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEmbeddings
from operator import itemgetter

# Core imports
from src.generation.openai_generator import get_llm
from src.retrieval.enhanced_retriever import (
    create_vector_only_retriever,
    create_hipporag_only_retriever, 
    format_qdrant_citation
)
from src.retrieval.query_transformer import create_query_transformer
from src.retrieval.reranker import create_reranker
from src.guardrails.injection_detector import is_prompt_injection
from src.guardrails.content_filter import is_sensitive_content
from src.routing import QueryRouter
from src.memory import MemoryManager
from src.persona.prompts import get_chat_prompt_template, SYSTEM_PROMPTS
from src.persona.admin_bot import AdminBot
from config import settings

# Query Types Enum for better classification
class QueryType(Enum):
    GREETING = "greeting"
    SIMPLE_LEGAL = "simple_legal"
    COMPLEX_LEGAL = "complex_legal"
    CONVERSATIONAL = "conversational"
    TECHNICAL = "technical"
    WEB_SEARCH = "web_search"


class ChatbotCore:
    def __init__(self):
        print("üöÄ Initializing Optimized Chatbot Core...")
        
        # Core models
        self.llm = get_llm()
        self.embeddings = HuggingFaceEmbeddings(model_name="AITeamVN/Vietnamese_Embedding")
        
        # Memory manager for legal queries only
        self.memory_manager = MemoryManager()
        
        # CSGT Bot for specialized commands (lazy loading)
        self._csgt_bot = None
        
        # Admin Bot for document management (EP-03)
        self.admin_bot = AdminBot(chatbot_core=self)
        
        # Query classification
        self.query_classifier = self._build_query_classifier()
        
        # Initialize retrievers (selective initialization)
        print("üìö Initializing retrievers...")
        self.vector_retriever = create_vector_only_retriever(self.embeddings)
        self.hipporag_retriever = create_hipporag_only_retriever()
        
        # Default retriever based on query type
        self.current_retriever = self.vector_retriever
        
        # Build optimized chains for different query types
        self.chains = self._build_specialized_chains()
        
        # Simple memory for follow-ups (no complex caching)
        self.conversation_context = {}
        
        print("‚úÖ Chatbot Core initialized!")

    @property
    def csgt_bot(self):
        """Lazy loading cho CSGTBot"""
        if self._csgt_bot is None:
            from src.persona.csgt_bot import CSGTBot
            self._csgt_bot = CSGTBot(self)
        return self._csgt_bot

    def _build_memory_validator(self):
        """Build LLM chain to validate if memory context can answer the question."""
        validator_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi.

NG·ªÆ C·∫¢NH T·ª™ B·ªò NH·ªö CU·ªòC TR√í CHUY·ªÜN:
{memory_context}

C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG: {question}

NHI·ªÜM V·ª§: ƒê√°nh gi√° xem ng·ªØ c·∫£nh tr√™n c√≥ ƒê·ª¶ TH√îNG TIN ƒë·ªÉ tr·∫£ l·ªùi C·ª§ TH·ªÇ v√† CH√çNH X√ÅC c√¢u h·ªèi kh√¥ng?

TI√äU CH√ç ƒê√ÅNH GI√Å:
‚úÖ C√ì ƒê·ª¶ TH√îNG TIN n·∫øu:
- Ng·ªØ c·∫£nh ch·ª©a TR·ª∞C TI·∫æP c√¢u tr·∫£ l·ªùi v·ªõi S·ªê LI·ªÜU C·ª§ TH·ªÇ (m·ª©c ph·∫°t, t·ªëc ƒë·ªô, v.v.)
- C√¢u h·ªèi l√† "t·ªïng c·ªông/c·ªông l·∫°i" v√† ng·ªØ c·∫£nh c√≥ ƒê·∫¶Y ƒê·ª¶ c√°c con s·ªë c·∫ßn t√≠nh
- C√¢u h·ªèi h·ªèi l·∫°i v·ªÅ ƒëi·ªÅu ƒë√£ ƒë∆∞·ª£c n√≥i tr∆∞·ªõc ƒë√≥ trong ng·ªØ c·∫£nh

‚ùå KH√îNG ƒê·ª¶ TH√îNG TIN n·∫øu:
- Ng·ªØ c·∫£nh ch·ªâ c√≥ c√¢u h·ªèi, KH√îNG c√≥ c√¢u tr·∫£ l·ªùi
- Ng·ªØ c·∫£nh tr·∫£ l·ªùi M∆† H·ªí, kh√¥ng c√≥ s·ªë li·ªáu c·ª• th·ªÉ
- C√¢u h·ªèi y√™u c·∫ßu th√¥ng tin M·ªöI ho√†n to√†n kh√¥ng c√≥ trong ng·ªØ c·∫£nh
- Ng·ªØ c·∫£nh c√≥ c√¢u tr·∫£ l·ªùi nh∆∞ng KH√îNG LI√äN QUAN ƒë·∫øn c√¢u h·ªèi hi·ªán t·∫°i

V√ç D·ª§:
1. Ng·ªØ c·∫£nh: "M·ª©c ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm l√† 400k-600k"
   C√¢u h·ªèi: "M·ª©c ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm l√† bao nhi√™u?"
   ‚Üí C√ì ƒê·ª¶ (tr·ª±c ti·∫øp tr·∫£ l·ªùi v·ªõi s·ªë c·ª• th·ªÉ)

2. Ng·ªØ c·∫£nh: "M·ª©c ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm?" (ch·ªâ l√† c√¢u h·ªèi)
   C√¢u h·ªèi: "M·ª©c ph·∫°t l√† bao nhi√™u?"
   ‚Üí KH√îNG ƒê·ª¶ (ng·ªØ c·∫£nh kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi)

3. Ng·ªØ c·∫£nh: "Ph·∫°t m≈©: 400k-600k\nPh·∫°t b·∫±ng l√°i: 4M-6M"
   C√¢u h·ªèi: "V·∫≠y t·ªïng c·ªông bao nhi√™u?"
   ‚Üí C√ì ƒê·ª¶ (c√≥ ƒë·ªß s·ªë li·ªáu ƒë·ªÉ t√≠nh t·ªïng)

4. Ng·ªØ c·∫£nh: "Ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm"
   C√¢u h·ªèi: "C√≤n n·∫øu kh√¥ng c√≥ b·∫±ng l√°i th√¨ sao?"
   ‚Üí KH√îNG ƒê·ª¶ (c√¢u h·ªèi v·ªÅ th√¥ng tin m·ªõi kh√¥ng c√≥ trong ng·ªØ c·∫£nh)

CH·ªà TR·∫¢ L·ªúI B·∫∞NG M·ªòT T·ª™: "YES" (c√≥ ƒë·ªß) ho·∫∑c "NO" (kh√¥ng ƒë·ªß)"""
        )
        
        return validator_prompt | self.llm | StrOutputParser()

    def _build_query_classifier(self):
        """Build smart query classifier."""
        classifier_prompt = ChatPromptTemplate.from_template(
            """Ph√¢n lo·∫°i c√¢u h·ªèi sau v√†o m·ªôt trong c√°c lo·∫°i:
            
1. GREETING: CH·ªà nh·ªØng c√¢u ch√†o h·ªèi thu·∫ßn t√∫y nh∆∞ "xin ch√†o", "hello", "hi", "c·∫£m ∆°n", "thank you". KH√îNG bao g·ªìm c√¢u c√≥ √Ω ƒë·ªãnh h·ªèi th√¥ng tin.
2. SIMPLE_LEGAL: H·ªèi ph√°p lu·∫≠t ƒë∆°n gi·∫£n, C·ª§ TH·ªÇ (v√≠ d·ª•: "m·ª©c ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm l√† bao nhi√™u?", "t·ªëc ƒë·ªô t·ªëi ƒëa trong khu d√¢n c∆∞?")
3. COMPLEX_LEGAL: H·ªèi ph√°p lu·∫≠t PH·ª®C T·∫†P (so s√°nh gi·ªØa c√°c nƒÉm, nhi·ªÅu ƒëi·ªÅu ki·ªán k·∫øt h·ª£p, ph√¢n t√≠ch s√¢u)
4. CONVERSATIONAL: Tr√≤ chuy·ªán chung chung, KH√îNG C·ª§ TH·ªÇ, KH√îNG h·ªèi v·ªÅ quy ƒë·ªãnh/m·ª©c ph·∫°t c·ª• th·ªÉ
5. WEB_SEARCH: H·ªèi v·ªÅ "quy ƒë·ªãnh M·ªöI NH·∫§T", "thay ƒë·ªïi g·∫ßn ƒë√¢y", "tin t·ª©c", "hi·ªán nay"

QUY T·∫ÆC QUAN TR·ªåNG:
- N·∫øu c√¢u h·ªèi c√≥ "m·ª©c ph·∫°t", "quy ƒë·ªãnh", "lu·∫≠t" + n·ªôi dung C·ª§ TH·ªÇ -> SIMPLE_LEGAL
- N·∫øu h·ªèi v·ªÅ "quy ƒë·ªãnh m·ªõi nh·∫•t", "thay ƒë·ªïi g√¨" NH∆ØNG KH√îNG c√≥ n·ªôi dung c·ª• th·ªÉ -> WEB_SEARCH
- "Xin ch√†o, t√¥i mu·ªën h·ªèi v·ªÅ X" -> CONVERSATIONAL (v√¨ ch·ªâ c√≥ √Ω ƒë·ªãnh h·ªèi chung chung)
- "T√¥i ƒëang quan t√¢m v·ªÅ quy ƒë·ªãnh m·ªõi nh·∫•t" -> WEB_SEARCH (v√¨ chung chung, c·∫ßn t√¨m ki·∫øm)

C√¢u h·ªèi: "{question}"

Ch·ªâ tr·∫£ l·ªùi b·∫±ng M·ªòT T·ª™: GREETING, SIMPLE_LEGAL, COMPLEX_LEGAL, CONVERSATIONAL, WEB_SEARCH"""
        )
        
        return classifier_prompt | self.llm | StrOutputParser()
    
    def _build_specialized_chains(self):
        """Build specialized chains for different query types."""
        chains = {}
        
        # Greeting chain (no retrieval needed)
        chains[QueryType.GREETING] = self._build_greeting_chain()
        
        # Simple legal chain (MEMORY FIRST -> vector search -> web search -> hipporag)
        chains[QueryType.SIMPLE_LEGAL] = self._build_simple_legal_chain()
        
        # Complex legal chain (HippoRAG + vector)
        chains[QueryType.COMPLEX_LEGAL] = self._build_complex_legal_chain()
        
        # Conversational chain (minimal retrieval)
        chains[QueryType.CONVERSATIONAL] = self._build_conversational_chain()
        
        # Web search chain (external search)
        chains[QueryType.WEB_SEARCH] = self._build_web_search_chain()
        
        return chains
    
    def _build_greeting_chain(self):
        """Build chain for greetings and simple interactions with persona support."""
        def persona_greeting(inputs):
            question = inputs["question"]
            persona_key = inputs.get("persona_key", "default")
            
            # Get system prompt from prompts.py
            sys_content = SYSTEM_PROMPTS.get(persona_key, SYSTEM_PROMPTS["default"])
            
            greeting_prompt = ChatPromptTemplate.from_template(
                f"""{sys_content}

Ng∆∞·ªùi d√πng: {{question}}

Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ph√π h·ª£p v·ªõi vai tr√≤."""
            )
            
            chain = greeting_prompt | self.llm | StrOutputParser()
            return chain.invoke({"question": question})
        
        return RunnableLambda(persona_greeting)
    
    def _build_simple_legal_chain(self):
        simple_legal_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† chuy√™n gia lu·∫≠t giao th√¥ng Vi·ªát Nam. Tr·∫£ l·ªùi CH√çNH X√ÅC d·ª±a tr√™n th√¥ng tin c√≥ s·∫µn.
            
TH√îNG TIN:
{context}
            
C√ÇU H·ªéI: {question}
            
H√ÉY TR·∫¢ L·ªúI:
- N·∫øu th√¥ng tin ƒë·ªß: ƒê∆∞a ra c√¢u tr·∫£ l·ªùi C·ª§ TH·ªÇ v√† R√ï R√ÄNG (s·ªë ti·ªÅn ph·∫°t, t·ªëc ƒë·ªô, v.v.)
- N·∫øu c·∫ßn T√çNH TO√ÅN (nh∆∞ "t·ªïng c·ªông"): H√£y T√çNH TO√ÅN v√† ƒë∆∞a ra k·∫øt qu·∫£ c·ª• th·ªÉ
- Tr√≠ch d·∫´n ngu·ªìn ph√°p l√Ω n·∫øu c√≥ (Ngh·ªã ƒë·ªãnh, ƒêi·ªÅu, Kho·∫£n)
- Gi·ªçng nh∆∞ cu·ªôc tr√≤ chuy·ªán t·ª± nhi√™n, NG·∫ÆN G·ªåN

L∆ØU √ù: N·∫øu th√¥ng tin KH√îNG ƒë·ªß, h√£y n√≥i th·∫≥ng "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ v·ªÅ {question} trong c∆° s·ªü d·ªØ li·ªáu."""
        )
        """Build chain with MEMORY as FIRST priority retriever and persona support."""
        def get_persona_prompt(persona_key: str) -> ChatPromptTemplate:
            # Use get_chat_prompt_template from prompts.py
            return get_chat_prompt_template(persona_key)
        
        def persona_legal_response(inputs):
            question = inputs["question"]
            context = inputs["context"]
            persona_key = inputs.get("persona_key", "default")
            
            # Get persona-specific prompt
            prompt_template = get_persona_prompt(persona_key)
            
            # Apply the prompt with context and question
            chain = prompt_template | self.llm | StrOutputParser()
            return chain.invoke({"context": context, "question": question})
        
#         def smart_retrieval_with_memory_first(inputs):
#             """MEMORY FIRST retrieval strategy with strict validation."""
#             question = inputs["question"]
#             user_id = inputs.get("user_id", "")
            
#             # ‚úÖ STEP 0: Try MEMORY FIRST (highest priority)
#             if user_id:
#                 print("üß† Checking if MEMORY can answer the question...")
                
#                 try:
#                     memory_context = self.memory_manager.get_context(user_id, limit=5)
                    
#                     if memory_context and memory_context.strip():
#                         # Use memory manager's validator
#                         is_sufficient = self.memory_manager.validate_memory_sufficiency(
#                             memory_context, question
#                         )
                        
#                         if is_sufficient:
#                             print("‚úÖ MEMORY has sufficient information! Using memory directly.")
                            
#                             # Generate answer from memory using LLM
#                             memory_answer_prompt = ChatPromptTemplate.from_template(
#                                 """D·ª±a v√†o NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch T·ª∞ NHI√äN v√† C·ª§ TH·ªÇ.

# NG·ªÆ C·∫¢NH:
# {memory_context}

# C√ÇU H·ªéI: {question}

# Y√äU C·∫¶U:
# - N·∫øu c·∫ßn T√çNH TO√ÅN (nh∆∞ "t·ªïng c·ªông"), h√£y T√çNH v√† ƒë∆∞a ra K·∫æT QU·∫¢ C·ª§ TH·ªÇ
# - Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, gi·ªçng ƒëi·ªáu T·ª∞ NHI√äN nh∆∞ ƒëang tr√≤ chuy·ªán
# - KH√îNG c·∫ßn tr√≠ch d·∫´n ngu·ªìn v√¨ ƒë√¢y l√† th√¥ng tin t·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc
# - CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong ng·ªØ c·∫£nh

# TR·∫¢ L·ªúI:"""
#                             )
                            
#                             answer_chain = memory_answer_prompt | self.llm | StrOutputParser()
#                             memory_answer = answer_chain.invoke({
#                                 "memory_context": memory_context,
#                                 "question": question
#                             })
                            
#                             return f"[T·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc]\n{memory_answer}"
#                         else:
#                             print("‚ö†Ô∏è MEMORY validation failed - information insufficient or irrelevant")
#                     else:
#                         print("‚ÑπÔ∏è No memory context available")
                        
#                 except Exception as e:
#                     print(f"‚ùå Memory retrieval error: {e}")
#             else:
#                 print("‚ÑπÔ∏è No user_id provided, skipping memory retrieval")
            
#             # Helper function to check if response is insufficient
#             def is_insufficient_response(response_text):
#                 insufficient_indicators = [
#                     "kh√¥ng th·ªÉ x√°c ƒë·ªãnh", "kh√¥ng c√≥ th√¥ng tin", "kh√¥ng t√¨m th·∫•y",
#                     "d·ª±a tr√™n t√†i li·ªáu", "do kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu",
#                     "t√¥i kh√¥ng th·ªÉ tr√≠ch d·∫´n", "kh√¥ng th·ªÉ n√™u r√µ", "kh√¥ng c√≥ ƒëi·ªÅu kho·∫£n", "r·∫•t ti·∫øc",
#                     "b·∫°n c·∫ßn tham kh·∫£o", "kh√¥ng ƒë·ªÅ c·∫≠p", "th√¥ng tin b·∫°n cung c·∫•p kh√¥ng",
#                     "c√°c vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t kh√°c", "ch·ªâ quy ƒë·ªãnh chung"
#                 ]
#                 response_lower = response_text.lower()
                
#                 has_indicator = any(indicator in response_lower for indicator in insufficient_indicators)
#                 has_specific_info = any(char.isdigit() for char in response_text)
                
#                 return has_indicator or not has_specific_info
            
#             # ‚úÖ STEP 1: Try Vector retriever
#             try:
#                 query_transformer = create_query_transformer(self.vector_retriever, self.llm)
#                 reranker = create_reranker(query_transformer)
#                 vector_docs = reranker.invoke(question)
                
#                 if vector_docs and len(vector_docs) > 0:
#                     print("üìö Trying vector retriever")
#                     vector_context = []
#                     for doc in vector_docs[:3]:
#                         content = doc.page_content
#                         metadata = doc.metadata
#                         citation = format_qdrant_citation(metadata)
#                         vector_context.append(f"{content}\n[Ngu·ªìn: {citation}]")
                    
#                     return "\n\n".join(vector_context)
                
#                 # check if vector_context is sufficient
#                 vector_formatted = "\n\n".join(vector_context)
#                 test_response = self.llm.invoke(
#                     simple_legal_prompt.format(context=vector_formatted, question=question)
#                 )
#             except Exception as e:
#                 print(f"‚ùå Vector retriever error: {e}")
            
#             return "Xin l·ªói, kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c v·ªÅ c√¢u h·ªèi n√†y."
        
#         return (
#             {
#                 "question": itemgetter("question"),
#                 "user_id": itemgetter("user_id"),
#                 "context": RunnableLambda(smart_retrieval_with_memory_first)
#             }
#             | RunnableLambda(lambda inputs: {
#                 "context": inputs["context"],
#                 "question": inputs["question"],
#                 "persona_key": inputs.get("persona_key", "default")
#             })
#             | RunnableLambda(persona_legal_response)
#         )
        
        def smart_retrieval_with_memory_first(inputs):
            """MEMORY FIRST retrieval strategy with strict validation."""
            question = inputs["question"]
            user_id = inputs.get("user_id", "")
            
            # ‚úÖ STEP 0: Try MEMORY FIRST (highest priority)
            if user_id:
                print("üß† Checking if MEMORY can answer the question...")
                
                try:
                    memory_context = self.memory_manager.get_context(user_id, limit=5)
                    print(f"üß† Retrieved memory context: {memory_context}")

                    if memory_context and memory_context.strip():
                        # Use memory manager's validator
                        is_sufficient = self.memory_manager.validate_memory_sufficiency(
                            memory_context, question
                        )
                        
                        if is_sufficient:
                            print("‚úÖ MEMORY has sufficient information! Using memory directly.")
                            
                            # Generate answer from memory using LLM
                            memory_answer_prompt = ChatPromptTemplate.from_template(
                                """D·ª±a v√†o NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch T·ª∞ NHI√äN v√† C·ª§ TH·ªÇ.

NG·ªÆ C·∫¢NH:
{memory_context}

C√ÇU H·ªéI: {question}

Y√äU C·∫¶U:
- N·∫øu c·∫ßn T√çNH TO√ÅN (nh∆∞ "t·ªïng c·ªông"), h√£y T√çNH v√† ƒë∆∞a ra K·∫æT QU·∫¢ C·ª§ TH·ªÇ
- Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, gi·ªçng ƒëi·ªáu T·ª∞ NHI√äN nh∆∞ ƒëang tr√≤ chuy·ªán
- KH√îNG c·∫ßn tr√≠ch d·∫´n ngu·ªìn v√¨ ƒë√¢y l√† th√¥ng tin t·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc
- CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong ng·ªØ c·∫£nh

TR·∫¢ L·ªúI:"""
                            )
                            
                            answer_chain = memory_answer_prompt | self.llm | StrOutputParser()
                            memory_answer = answer_chain.invoke({
                                "memory_context": memory_context,
                                "question": question
                            })
                            
                            return f"[T·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc]\n{memory_answer}"
                        else:
                            print("‚ö†Ô∏è MEMORY validation failed - information insufficient or irrelevant")
                    else:
                        print("‚ÑπÔ∏è No memory context available")
                        
                except Exception as e:
                    print(f"‚ùå Memory retrieval error: {e}")
            else:
                print("‚ÑπÔ∏è No user_id provided, skipping memory retrieval")
            
            # Helper function to check if response is insufficient
            def is_insufficient_response(response_text):
                insufficient_indicators = [
                    "kh√¥ng th·ªÉ x√°c ƒë·ªãnh", "kh√¥ng c√≥ th√¥ng tin", "kh√¥ng t√¨m th·∫•y",
                    "d·ª±a tr√™n t√†i li·ªáu", "do kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu",
                    "t√¥i kh√¥ng th·ªÉ tr√≠ch d·∫´n", "kh√¥ng th·ªÉ n√™u r√µ", "kh√¥ng c√≥ ƒëi·ªÅu kho·∫£n", "r·∫•t ti·∫øc",
                    "b·∫°n c·∫ßn tham kh·∫£o", "kh√¥ng ƒë·ªÅ c·∫≠p", "th√¥ng tin b·∫°n cung c·∫•p kh√¥ng",
                    "c√°c vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t kh√°c", "ch·ªâ quy ƒë·ªãnh chung"
                ]
                response_lower = response_text.lower()
                
                has_indicator = any(indicator in response_lower for indicator in insufficient_indicators)
                has_specific_info = any(char.isdigit() for char in response_text)
                
                return has_indicator or not has_specific_info
            
            # ‚úÖ STEP 1: Try Vector retriever
            try:
                query_transformer = create_query_transformer(self.vector_retriever, self.llm)
                reranker = create_reranker(query_transformer)
                vector_docs = reranker.invoke(question)
                
                if vector_docs and len(vector_docs) > 0:
                    print("üìö Trying vector retriever")
                    vector_context = []
                    for doc in vector_docs[:3]:
                        content = doc.page_content
                        metadata = doc.metadata
                        citation = format_qdrant_citation(metadata)
                        vector_context.append(f"{content}\n[Ngu·ªìn: {citation}]")
                    
                    vector_formatted = "\n\n".join(vector_context)
                    
                    test_response = self.llm.invoke(
                        simple_legal_prompt.format(context=vector_formatted, question=question)
                    )
                    
                    response_text = test_response.content if hasattr(test_response, 'content') else str(test_response)
                    
                    if not is_insufficient_response(response_text):
                        print("‚úÖ Vector retriever provided sufficient answer")
                        return vector_formatted
                    else:
                        print("‚ö†Ô∏è Vector answer insufficient, trying web search")
            except Exception as e:
                print(f"‚ùå Vector retriever error: {e}")
            
            # ‚úÖ STEP 2: Try Web search
            try:
                print("üåê Falling back to web search")
                from src.retrieval.web_search import get_web_search_tool
                web_tool = get_web_search_tool()
                web_results = web_tool.invoke(question)
                
                if web_results:
                    web_content = "\n".join([f"- {r.get('content', '')[:300]}" for r in web_results[:3]])
                    web_context = f"Th√¥ng tin t√¨m ki·∫øm tr√™n web:\n{web_content}"
                    
                    test_response = self.llm.invoke(
                        simple_legal_prompt.format(context=web_context, question=question)
                    )
                    
                    response_text = test_response.content if hasattr(test_response, 'content') else str(test_response)
                    
                    if not is_insufficient_response(response_text):
                        print("‚úÖ Web search provided sufficient answer")
                        return web_context
                    else:
                        print("‚ö†Ô∏è Web search answer insufficient, trying HippoRAG")
            except Exception as e:
                print(f"‚ùå Web search error: {e}")
            
            # ‚úÖ STEP 3: Try HippoRAG as last resort
            try:
                print("ü¶Ñ Falling back to HippoRAG")
                hippo_docs = self.hipporag_retriever.invoke(question)
                if hippo_docs:
                    hippo_context = "\n".join([doc.page_content for doc in hippo_docs[:2]])
                    print("‚úÖ Using HippoRAG as final fallback")
                    return hippo_context
            except Exception as e:
                print(f"‚ùå HippoRAG error: {e}")
            
            return "Xin l·ªói, kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c v·ªÅ c√¢u h·ªèi n√†y."
        
        return (
            {
                "question": itemgetter("question"),
                "user_id": itemgetter("user_id"),
                "context": RunnableLambda(smart_retrieval_with_memory_first)
            }
            | simple_legal_prompt
            | self.llm
            | StrOutputParser()
        )
    
    def _build_complex_legal_chain(self):
        """Build chain for complex legal queries using HippoRAG + vector with memory support."""
        complex_legal_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† chuy√™n gia lu·∫≠t cao c·∫•p, gi·ªèi ph√¢n t√≠ch ph·ª©c t·∫°p v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam.
            
TH√îNG TIN T·ª™ KI·∫æN TH·ª®C GRAPH:
{hippo_context}
            
TH√îNG TIN T·ª™ T√ÄI LI·ªÜU:
{vector_context}
            
C√ÇU H·ªéI PH·ª®C T·∫†P: {question}
            
H√ÉY:
1. K·∫æT H·ª¢P th√¥ng tin t·ª´ c·∫£ hai ngu·ªìn
2. PH√ÇN T√çCH so s√°nh, li√™n k·∫øt
3. ƒê∆ØA RA k·∫øt lu·∫≠n r√µ r√†ng
4. TR√çCH D·∫™N ƒë·∫ßy ƒë·ªß cƒÉn c·ª© ph√°p l√Ω
            
TR·∫¢ L·ªúI CHI TI·∫æT:"""
        )
        
        vector_transformer = create_query_transformer(self.vector_retriever, self.llm)
        vector_reranker = create_reranker(vector_transformer)
        
        def format_contexts(inputs):
            question = inputs["question"]
            user_id = inputs.get("user_id", "")
            
            # Check memory first for complex queries too
            if user_id:
                try:
                    memory_context = self.memory_manager.get_context(user_id, limit=5)
                    if memory_context and self.memory_manager.validate_memory_sufficiency(memory_context, question):
                        print("‚úÖ MEMORY sufficient for complex query")
                        return {
                            "question": question,
                            "vector_context": f"[T·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc]\n{memory_context}",
                            "hippo_context": ""
                        }
                except Exception as e:
                    print(f"‚ùå Memory error in complex chain: {e}")
            
            vector_docs = vector_reranker.invoke(question)
            vector_context = "\n".join([doc.page_content for doc in vector_docs[:3]])
            
            try:
                hippo_docs = self.hipporag_retriever.invoke(question)
                hippo_context = "\n".join([doc.page_content for doc in hippo_docs[:3]])
            except Exception as e:
                print(f"HippoRAG error: {e}")
                hippo_context = "Kh√¥ng c√≥ th√¥ng tin t·ª´ knowledge graph"
            
            return {
                "question": question,
                "vector_context": vector_context,
                "hippo_context": hippo_context
            }
        
        return (
            RunnableLambda(format_contexts)
            | complex_legal_prompt
            | self.llm
            | StrOutputParser()
        )
    
    def _build_conversational_chain(self):
        """Build chain for conversational queries."""
        conv_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† tr·ª£ l√Ω AI th√¢n thi·ªán chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam.
            
Ng·ªØ c·∫£nh tr∆∞·ªõc: {previous_context}
            
Ng∆∞·ªùi d√πng: {question}
            
H√£y tr·∫£ l·ªùi t·ª± nhi√™n, th√¢n thi·ªán nh∆∞ cu·ªôc tr√≤ chuy·ªán b√¨nh th∆∞·ªùng. Ch·ªâ tr·∫£ l·ªùi chung chung, ng·∫Øn g·ªçn, kh√¥ng c·∫ßn tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t."""
        )
        
        return conv_prompt | self.llm | StrOutputParser()
    
    def _build_web_search_chain(self):
        """Build chain for web search queries."""
        web_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† tr·ª£ l√Ω AI t√¨m ki·∫øm th√¥ng tin m·ªõi nh·∫•t v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam.
            
K·∫øt qu·∫£ t√¨m ki·∫øm:
{search_results}
            
C√¢u h·ªèi: {question}
            
H√£y:
- LI·ªÜT K√ä NG·∫ÆN G·ªåN c√°c quy ƒë·ªãnh m·ªõi nh·∫•t (d·∫°ng bullet points)
- Tr√≠ch d·∫´n ngu·ªìn r√µ r√†ng
- KH√îNG gi·∫£i th√≠ch chi ti·∫øt
- Ch·ªâ th√¥ng tin quan tr·ªçng nh·∫•t"""
        )
        
        def web_search_and_format(inputs):
            question = inputs["question"]
            try:
                # Web search functionality - simplified for now
                search_results = f"T√¨m ki·∫øm web cho: {question}\n(Ch·ª©c nƒÉng web search s·∫Ω ƒë∆∞·ª£c tri·ªÉn khai sau)"
                return {"question": question, "search_results": search_results}
            except Exception as e:
                return {"question": question, "search_results": f"L·ªói t√¨m ki·∫øm: {e}"}
        
        return RunnableLambda(web_search_and_format) | web_prompt | self.llm | StrOutputParser()
    
    def process_query(self, question: str, user_id: str = None, persona_key: str = "default", force_hipporag: bool = False):
        """Enhanced query processing with MEMORY and CSGT commands support."""
        print(f"üü¢ process_query: {question=}, {user_id=}, {persona_key=}, {force_hipporag=}")
        
        try:
            # Step 0: Auto-enable HippoRAG for hipporag persona
            if persona_key == "hipporag":
                force_hipporag = True
                print("ü¶Ñ HippoRAG persona detected - automatically enabling force_hipporag=True")
            
            # Step 0: Force HippoRAG mode - bypass everything else, only use HippoRAG
            if force_hipporag:
                print("üéØ Force HippoRAG mode - using HippoRAG retriever only, no vector search")
                
                # Get documents from HippoRAG only
                hipporag_docs = self.hipporag_retriever.invoke(question)
                
                # Format HippoRAG results
                if hipporag_docs:
                    formatted_context = "\n\n".join([
                        f"üìÑ T√†i li·ªáu {i+1}: {format_qdrant_citation(doc.metadata)}\n{doc.page_content}"
                        for i, doc in enumerate(hipporag_docs)
                    ])
                else:
                    formatted_context = "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ph√π h·ª£p."
                
                # Use default persona prompt for HippoRAG-only response
                prompt_template = get_chat_prompt_template("default")
                chain = prompt_template | self.llm | StrOutputParser()
                
                response = chain.invoke({
                    "question": question,
                    "context": formatted_context
                })
                
                return response
            
            # Step 1: Check for CSGT special commands first
            if persona_key == "csgt" and question.strip().startswith("/"):
                return self._handle_csgt_commands(question.strip(), user_id)
            
            # Step 2: Classify query
            query_type = self._classify_query(question, [])
            print(f"üéØ Query classified as: {query_type.value}")
            
            # Step 2: Process with appropriate chain
            if query_type in self.chains:
                if query_type == QueryType.CONVERSATIONAL:
                    response = self.chains[query_type].invoke({
                        "question": question,
                        "previous_context": "",
                        "persona_key": persona_key
                    })
                elif query_type in [QueryType.SIMPLE_LEGAL, QueryType.COMPLEX_LEGAL]:
                    # ‚úÖ Pass user_id and persona_key to legal chains
                    response = self.chains[query_type].invoke({
                        "question": question,
                        "user_id": user_id or "",
                        "persona_key": persona_key
                    })
                else:
                    response = self.chains[query_type].invoke({
                        "question": question,
                        "persona_key": persona_key
                    })
            else:
                response = "Xin l·ªói, t√¥i kh√¥ng hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n."
            
            # Step 3: Add follow-up suggestions
            final_response = self._create_response(response, query_type, question)
            
            print("‚úÖ Query processed successfully")
            return final_response
            
        except Exception as e:
            import traceback
            print("‚ùå ERROR in process_query:")
            traceback.print_exc()
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {e}"

    def _handle_csgt_commands(self, command: str, user_id: str = None) -> str:
        """X·ª≠ l√Ω c√°c l·ªánh ƒë·∫∑c bi·ªát c·ªßa CSGT (AC1, AC2)"""
        try:
            if command.startswith("/lookup "):
                keyword = command[8:].strip()  # B·ªè "/lookup "
                return self.csgt_bot.fast_lookup(keyword)
            
            elif command.startswith("/checklist "):
                violation = command[11:].strip()  # B·ªè "/checklist "
                return self.csgt_bot.generate_checklist(violation)
            
            elif command.startswith("/quick "):
                code = command[7:].strip()  # B·ªè "/quick "
                return self.csgt_bot.quick_penalty_check(code)
            
            elif command.startswith("/draft"):
                # Ph√¢n t√≠ch th√¥ng tin t·ª´ command (ƒë∆°n gi·∫£n h√≥a)
                if ":" in command:
                    # Format: /draft name:X,plate:Y,violation:Z,...
                    parts = command[6:].strip().split(",")
                    info = {}
                    for part in parts:
                        if ":" in part:
                            key, value = part.split(":", 1)
                            info[key.strip()] = value.strip()
                    return self.csgt_bot.draft_report(info)
                else:
                    return """üìÑ ƒê·ªÉ so·∫°n bi√™n b·∫£n, vui l√≤ng cung c·∫•p th√¥ng tin:
/draft name:Nguy·ªÖn VƒÉn A,plate:29A-123.45,violation:V∆∞·ª£t ƒë√®n ƒë·ªè,time:10:30 21/11/2025,location:Ng√£ t∆∞ S·ªü"""
            
            elif command == "/help":
                return """üëÆ L·ªÜNH CSGT KH√ÅC NHAU:
/lookup [t·ª´ kh√≥a] - Tra c·ª©u nhanh m·ª©c ph·∫°t (‚â§10s)
/checklist [l·ªói] - T·∫°o checklist l·∫≠p bi√™n b·∫£n  
/quick [m√£] - Tra offline (VDR,QTS,KMB,NCN...)
/draft [info] - So·∫°n m·∫´u bi√™n b·∫£n
/help - Hi·ªÉn th·ªã menu n√†y

V√ç D·ª§:
/lookup v∆∞·ª£t ƒë√®n ƒë·ªè xe m√°y
/checklist n·ªìng ƒë·ªô c·ªìn
/quick VDR"""
            
            else:
                return f"‚ùå L·ªánh kh√¥ng h·ª£p l·ªá: {command}\nG√µ /help ƒë·ªÉ xem danh s√°ch l·ªánh."
                
        except Exception as e:
            return f"‚ùå L·ªói x·ª≠ l√Ω l·ªánh CSGT: {e}"
    
    def _handle_admin_commands(self, command: str, user_id: str = None) -> str:
        """X·ª≠ l√Ω c√°c l·ªánh admin qu·∫£n tr·ªã vƒÉn b·∫£n (EP-03)"""
        try:
            # US1: Document approval workflow
            if command == "/admin pending":
                return self.admin_bot.list_pending_documents()
            
            elif command.startswith("/admin approve "):
                filename = command[15:].strip()
                return self.admin_bot.approve_document(filename, approved_by=user_id or "admin")
            
            elif command.startswith("/admin reject "):
                parts = command[14:].split(maxsplit=1)
                filename = parts[0]
                reason = parts[1] if len(parts) > 1 else ""
                return self.admin_bot.reject_document(filename, reason)
            
            # US2/AC2: Version diff & status management
            elif command.startswith("/admin diff "):
                # Format: /admin diff <doc_name> <v1> <v2>
                parts = command[12:].split()
                if len(parts) >= 3:
                    return self.admin_bot.show_diff(parts[0], parts[1], parts[2])
                return "‚ùå Format: /admin diff <t√™n> <phi√™n_b·∫£n_c≈©> <phi√™n_b·∫£n_m·ªõi>"
            
            elif command.startswith("/admin status "):
                # Format: /admin status <doc_name> <status> [effective_date]
                parts = command[14:].split()
                if len(parts) >= 2:
                    doc_name = parts[0]
                    status = parts[1]
                    effective_date = parts[2] if len(parts) > 2 else None
                    return self.admin_bot.update_status(doc_name, status, effective_date)
                return "‚ùå Format: /admin status <t√™n> <tr·∫°ng_th√°i> [ng√†y_hi·ªáu_l·ª±c]"
            
            # AC1/AC3: Coverage & Indexing
            elif command == "/admin coverage":
                return self.admin_bot.check_coverage()
            
            elif command.startswith("/admin index "):
                filename = command[13:].strip()
                return self.admin_bot.trigger_indexing(filename)
            
            elif command == "/admin index-all":
                return self.admin_bot.trigger_indexing()
            
            # Stats & Help
            elif command == "/admin stats":
                return self.admin_bot.get_admin_stats()
            
            elif command in ["/admin help", "/admin"]:
                return self.admin_bot.get_help()
            
            else:
                return f"‚ùå L·ªánh admin kh√¥ng h·ª£p l·ªá: {command}\nG√µ /admin help ƒë·ªÉ xem danh s√°ch l·ªánh."
                
        except Exception as e:
            return f"‚ùå L·ªói x·ª≠ l√Ω l·ªánh admin: {e}"
    
    def _classify_query(self, question: str, chat_history: list) -> QueryType:
        """Classify query using LLM."""
        try:
            classification = self.query_classifier.invoke({"question": question})
            classification = classification.strip().upper()
            
            type_mapping = {
                "GREETING": QueryType.GREETING,
                "SIMPLE_LEGAL": QueryType.SIMPLE_LEGAL,
                "COMPLEX_LEGAL": QueryType.COMPLEX_LEGAL,
                "CONVERSATIONAL": QueryType.CONVERSATIONAL,
                "TECHNICAL": QueryType.CONVERSATIONAL,
                "WEB_SEARCH": QueryType.WEB_SEARCH
            }
            
            return type_mapping.get(classification, QueryType.CONVERSATIONAL)
        except Exception as e:
            print(f"‚ö†Ô∏è Classification error: {e}")
            return QueryType.CONVERSATIONAL
    
    def _create_response(self, content: str, query_type: QueryType, original_question: str = "") -> str:
        """Create formatted response with natural follow-up suggestions."""
        # Skip follow-ups if answer came from memory (starts with "[T·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc]")
        if content.startswith("[T·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc]"):
            return content
        
        if query_type in [QueryType.SIMPLE_LEGAL, QueryType.COMPLEX_LEGAL]:
            has_suggestions = any(phrase in content.lower() for phrase in ['c√¢u h·ªèi g·ª£i √Ω', 'follow-up', 'b·∫°n c√≥ th·ªÉ', 'n·∫øu b·∫°n'])
            is_insufficient = any(phrase in content.lower() for phrase in ['kh√¥ng t√¨m th·∫•y', 'kh√¥ng c√≥ th√¥ng tin', 'xin l·ªói'])
            is_too_short = len(content) < 200
            
            if not has_suggestions and not is_insufficient and not is_too_short:
                follow_ups = self._generate_follow_ups(query_type, original_question)
                if follow_ups:
                    return f"{content}\n\n{follow_ups}"
        return content
    
    def _generate_follow_ups(self, query_type: QueryType, original_question: str = "") -> str:
        """Generate natural follow-up suggestions using LLM."""
        followup_prompt = ChatPromptTemplate.from_template(
            """D·ª±a tr√™n c√¢u h·ªèi g·ªëc, h√£y t·∫°o th√™m m·ªôt g·ª£i √Ω ti·∫øp theo t·ª± nhi√™n v√† h·∫•p d·∫´n.
            
C√¢u h·ªèi g·ªëc: "{original_question}"
Lo·∫°i: {query_type}
            
T·∫°o c√°c c√¢u g·ª£i √Ω nh∆∞:
- "N·∫øu b·∫°n mu·ªën bi·∫øt th√™m v·ªÅ X, t√¥i c√≥ th·ªÉ gi·∫£i th√≠ch chi ti·∫øt h∆°n"
- "B·∫°n c≈©ng c√≥ th·ªÉ h·ªèi v·ªÅ Y n·∫øu quan t√¢m" 
- "T√¥i c≈©ng c√≥ th·ªÉ h∆∞·ªõng d·∫´n v·ªÅ Z n·∫øu b·∫°n c·∫ßn"
            
H√£y vi·∫øt t·ª± nhi√™n, th√¢n thi·ªán."""
        )
        
        try:
            followup_chain = followup_prompt | self.llm | StrOutputParser()
            follow_ups = followup_chain.invoke({
                "query_type": query_type.value,
                "original_question": original_question
            })
            return follow_ups.strip()
        except Exception as e:
            print(f"‚ö†Ô∏è Follow-up generation error: {e}")
            return "- N·∫øu b·∫°n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c v·ªÅ lu·∫≠t giao th√¥ng, t√¥i s·∫µn s√†ng gi·∫£i ƒë√°p!"
    
    def clear_context(self, user_id: str):
        """Clear conversation context for user."""
        if user_id in self.conversation_context:
            del self.conversation_context[user_id]
    
    def get_conversation_stats(self) -> Dict[str, int]:
        """Get conversation statistics."""
        return {
            "total_sessions": len(self.conversation_context),
            "query_types_supported": len(QueryType)
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get chatbot system information."""
        return {
            "query_types": [qt.value for qt in QueryType],
            "active_chains": list(self.chains.keys()),
            "conversation_sessions": len(self.conversation_context),
            "retrievers": {
                "memory": "Priority 0 - Conversation history",
                "vector": "Priority 1 - Qdrant vector store",
                "web": "Priority 2 - Web search",
                "hipporag": "Priority 3 - Knowledge graph"
            }
        }