# src/chatbot_core.py
from typing import Dict, Any, List, Optional
from enum import Enum
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEmbeddings
from operator import itemgetter

# Core imports
from src.generation.openai_generator import get_llm
from src.retrieval.enhanced_retriever import (
    create_vector_only_retriever,
    create_hipporag_only_retriever, 
    format_qdrant_citation
)
from src.retrieval.vietnamese_law_prompts import get_vietnamese_law_prompts
from src.guardrails.injection_detector import is_prompt_injection
from src.guardrails.content_filter import is_sensitive_content
from config import settings

# Query Types Enum for better classification
class QueryType(Enum):
    GREETING = "greeting"
    SIMPLE_LEGAL = "simple_legal"
    COMPLEX_LEGAL = "complex_legal"
    CONVERSATIONAL = "conversational"
    TECHNICAL = "technical"
    WEB_SEARCH = "web_search"


# STRICT_QA_SYSTEM_PROMPT - 5-section structured response format with citation enforcement
STRICT_QA_SYSTEM_PROMPT = """Báº¡n lÃ  AI trá»£ lÃ½ phÃ¡p lÃ½ chuyÃªn vá» phÃ¡p luáº­t giao thÃ´ng Viá»‡t Nam.

========================
NGUYÃŠN Táº®C Cá»T LÃ•I
========================
- CHá»ˆ Ä‘Æ°á»£c sá»­ dá»¥ng thÃ´ng tin cÃ³ trong CONTEXT Ä‘Æ°á»£c cung cáº¥p.
- TUYá»†T Äá»I khÃ´ng suy diá»…n, khÃ´ng bá»• sung kiáº¿n thá»©c ngoÃ i context.
- Náº¿u má»™t ná»™i dung (má»©c pháº¡t, tháº©m quyá»n, ngoáº¡i lá»‡, thá»§ tá»¥c, thá»i Ä‘iá»ƒm hiá»‡u lá»±c...)
  khÃ´ng cÃ³ cÄƒn cá»© rÃµ rÃ ng trong context â†’ pháº£i ghi rÃµ:
  "KhÃ´ng cÃ³ cÄƒn cá»© trong context Ä‘Æ°á»£c cung cáº¥p".
- Æ¯u tiÃªn tráº£ lá»i ngáº¯n gá»n, trá»±c tiáº¿p, Ä‘Ãºng trá»ng tÃ¢m.
- KhÃ´ng phÃ¢n tÃ­ch dÆ° thá»«a, khÃ´ng diá»…n giáº£i lan man.
- Chá»‰ viá»‡n dáº«n cÃ¡c Ä‘iá»u, khoáº£n, Ä‘iá»ƒm liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¢u há»i.

========================
BÆ¯á»šC 1 â€“ XÃC Äá»ŠNH LOáº I CÃ‚U Há»ŽI PHÃP LÃ
========================
TrÆ°á»›c khi tráº£ lá»i, pháº£i xÃ¡c Ä‘á»‹nh cÃ¢u há»i thuá»™c Má»˜T (hoáº·c nhiá»u) nhÃ³m sau:

(1) Tháº©m quyá»n xá»­ lÃ½ THEO HÃ€NH VI vi pháº¡m
    â†’ Há»i: â€œÄ‘Æ°á»£c xá»­ lÃ½ nhá»¯ng hÃ nh vi nÃ o?â€, â€œÄ‘Æ°á»£c xá»­ lÃ½ lá»—i gÃ¬?â€

(2) Tháº©m quyá»n xá»­ lÃ½ THEO Má»¨C Xá»¬ PHáº T
    â†’ Há»i: â€œÄ‘Æ°á»£c pháº¡t bao nhiÃªu tiá»n?â€, â€œcÃ³ Ä‘Æ°á»£c tÆ°á»›c GPLX, tá»‹ch thu khÃ´ng?â€

(3) Má»©c pháº¡t / háº­u quáº£ phÃ¡p lÃ½ Ä‘á»‘i vá»›i hÃ nh vi cá»¥ thá»ƒ

(4) TrÃ¬nh tá»±, thá»§ tá»¥c xá»­ pháº¡t / trá»« Ä‘iá»ƒm / phá»¥c há»“i Ä‘iá»ƒm

(5) TrÆ°á»ng há»£p ngoáº¡i lá»‡, Ä‘iá»u kiá»‡n khÃ´ng bá»‹ xá»­ pháº¡t

========================
BÆ¯á»šC 2 â€“ GIá»šI Háº N ÄIá»€U LUáº¬T ÄÆ¯á»¢C PHÃ‰P VIá»†N DáºªN
========================
- Náº¿u cÃ¢u há»i thuá»™c nhÃ³m (1) â€“ THEO HÃ€NH VI:
  + CHá»ˆ Ä‘Æ°á»£c viá»‡n dáº«n cÃ¡c Ä‘iá»u luáº­t phÃ¢n Ä‘á»‹nh tháº©m quyá»n theo hÃ nh vi
    (vÃ­ dá»¥: Äiá»u 41 Nghá»‹ Ä‘á»‹nh 168/2024).
  + KHÃ”NG viá»‡n dáº«n cÃ¡c Ä‘iá»u quy Ä‘á»‹nh tháº©m quyá»n theo má»©c tiá»n, má»©c pháº¡t chung
    (vÃ­ dá»¥: Äiá»u 43), trá»« khi cÃ¢u há»i há»i rÃµ thÃªm vá» má»©c xá»­ pháº¡t.

- Náº¿u cÃ¢u há»i thuá»™c nhÃ³m (2) â€“ THEO Má»¨C Xá»¬ PHáº T:
  + Æ¯u tiÃªn viá»‡n dáº«n cÃ¡c Ä‘iá»u quy Ä‘á»‹nh tháº©m quyá»n xá»­ pháº¡t theo má»©c tiá»n,
    hÃ¬nh thá»©c xá»­ pháº¡t bá»• sung (vÃ­ dá»¥: Äiá»u 43).
  + KhÃ´ng liá»‡t kÃª chi tiáº¿t tá»«ng hÃ nh vi náº¿u khÃ´ng cáº§n thiáº¿t.

- Náº¿u cÃ¢u há»i thuá»™c nhiá»u nhÃ³m:
  + Pháº£i tÃ¡ch rÃµ tá»«ng ná»™i dung tÆ°Æ¡ng á»©ng vá»›i tá»«ng nhÃ³m.
  + Má»—i nhÃ³m sá»­ dá»¥ng Ä‘Ãºng loáº¡i Ä‘iá»u luáº­t tÆ°Æ¡ng á»©ng.

========================
YÃŠU Cáº¦U Ná»˜I DUNG TRáº¢ Lá»œI
========================
1. Tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m cÃ¢u há»i, khÃ´ng tráº£ lá»i thay cÃ¢u há»i khÃ¡c.
2. NÃªu rÃµ (náº¿u cÃ³ trong context):
   - Quy Ä‘á»‹nh phÃ¡p luáº­t Ä‘ang Ã¡p dá»¥ng
   - Má»©c xá»­ pháº¡t / háº­u quáº£ phÃ¡p lÃ½
   - HÃ¬nh thá»©c xá»­ pháº¡t bá»• sung
   - TrÆ°á»ng há»£p ngoáº¡i lá»‡
3. Má»—i káº¿t luáº­n phÃ¡p lÃ½ PHáº¢I:
   - CÃ³ citation rÃµ rÃ ng: (Äiá»u â€“ Khoáº£n â€“ Äiá»ƒm â€“ TÃªn vÄƒn báº£n)
   - TrÃ­ch ngáº¯n gá»n Ä‘Ãºng pháº§n ná»™i dung cá»§a citation liÃªn quan trá»±c tiáº¿p.
4. KhÃ´ng Ä‘Æ°á»£c trÃ­ch dáº«n sai Ä‘iá»u, sai khoáº£n, sai pháº¡m vi Ã¡p dá»¥ng.
5. KhÃ´ng sá»­ dá»¥ng cÃ¡c tá»« ngá»¯ suy Ä‘oÃ¡n:
   â€œcÃ³ thá»ƒâ€, â€œthÆ°á»ngâ€, â€œtrong thá»±c táº¿â€, â€œtheo thÃ´ng lá»‡â€.

========================
YÃŠU Cáº¦U Vá»€ HÃŒNH THá»¨C TRáº¢ Lá»œI
========================
CÃ¢u tráº£ lá»i PHáº¢I cÃ³ Ä‘á»§ cÃ¡c pháº§n sau:

I. Tráº£ lá»i
   - Ngáº¯n gá»n, cá»¥ thá»ƒ, Ä‘i tháº³ng vÃ o ná»™i dung chÃ­nh cá»§a cÃ¢u há»i.

II. Má»©c xá»­ pháº¡t / Háº­u quáº£ phÃ¡p lÃ½ (náº¿u cÃ³)

III. TrÆ°á»ng há»£p ngoáº¡i lá»‡ (náº¿u cÃ³; náº¿u khÃ´ng cÃ³ thÃ¬ ghi rÃµ khÃ´ng cÃ³ cÄƒn cá»©)

IV. Khuyáº¿n nghá»‹ cho ngÆ°á»i há»i
   - Chá»‰ mang tÃ­nh tuÃ¢n thá»§ phÃ¡p luáº­t, an toÃ n giao thÃ´ng.
   - KhÃ´ng tÆ° váº¥n nÃ© trÃ¡nh xá»­ pháº¡t, khÃ´ng Ä‘Æ°a máº¹o Ä‘á»‘i phÃ³ cÆ¡ quan chá»©c nÄƒng.

V. CÄƒn cá»© phÃ¡p lÃ½
   - Liá»‡t kÃª Ä‘áº§y Ä‘á»§, chÃ­nh xÃ¡c cÃ¡c Ä‘iá»u khoáº£n Ä‘Ã£ viá»‡n dáº«n.

========================
Káº¾T THÃšC CÃ‚U TRáº¢ Lá»œI
========================
Pháº£i cÃ³ Ä‘oáº¡n *LÆ°u Ã½* vá»›i ná»™i dung:
"Ná»™i dung do AI tá»•ng há»£p tá»« vÄƒn báº£n phÃ¡p luáº­t Ä‘Æ°á»£c cung cáº¥p, chá»‰ cÃ³ giÃ¡ trá»‹ tham kháº£o,
khÃ´ng thay tháº¿ Ã½ kiáº¿n tÆ° váº¥n phÃ¡p lÃ½ chÃ­nh thá»©c cá»§a luáº­t sÆ° hoáº·c cÆ¡ quan cÃ³ tháº©m quyá»n."
"""


class ChatbotCore:
    def __init__(self, use_memory: bool = False):
        """
        Initialize Optimized Chatbot Core
        
        Args:
            use_memory: Náº¿u True, sá»­ dá»¥ng memory Ä‘á»ƒ lÆ°u vÃ  truy xuáº¥t conversation history.
                       Náº¿u False, táº¯t táº¥t cáº£ memory functionality (default: False)
        """
        print("ðŸš€ Initializing Optimized Chatbot Core...")
        print(f"   Memory: {'ENABLED' if use_memory else 'DISABLED'}")
        
        # Store memory flag
        self.use_memory = use_memory
        
        # Core models - OpenAI client (not langchain)
        self.openai_client = get_llm()
        self.embeddings = HuggingFaceEmbeddings(model_name="AITeamVN/Vietnamese_Embedding")
        
        # Initialize retrievers (selective initialization)
        print("ðŸ“š Initializing retrievers...")
        self.vector_retriever = create_vector_only_retriever(self.embeddings)
        self.hipporag_retriever = create_hipporag_only_retriever()
        
        # Default retriever based on query type
        self.current_retriever = self.vector_retriever
        
        # Build optimized chains for different query types
        self.chains = self._build_specialized_chains()
        
        # Simple memory for follow-ups (only if memory is enabled)
        if self.use_memory:
            self.conversation_context = {}
        else:
            self.conversation_context = None
        
        print("âœ… Chatbot Core initialized!")
    
    def _call_openai(self, messages: list, max_tokens: int = 3000) -> str:
        """
        Helper method to call OpenAI API directly
        Replacement for langchain's LLM invocation
        """
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_completion_tokens=max_tokens,
                temperature=0
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"âŒ OpenAI API call failed: {e}")
            return f"Lá»—i khi gá»i API: {str(e)}"
    
    def _build_specialized_chains(self):
        """Build specialized chains for different query types."""
        chains = {}
        
        # Greeting chain (no retrieval needed)
        chains[QueryType.GREETING] = self._build_greeting_chain()
        
        # HippoRAG chain - PRIMARY chain cho ALL legal queries (retrieve 60 docs + strict prompt)
        hipporag_chain = self._build_hipporag_chain()
        chains[QueryType.SIMPLE_LEGAL] = hipporag_chain
        chains[QueryType.COMPLEX_LEGAL] = hipporag_chain
        
        # Conversational chain (minimal retrieval)
        chains[QueryType.CONVERSATIONAL] = self._build_conversational_chain()
        
        # Web search chain (external search)
        chains[QueryType.WEB_SEARCH] = self._build_web_search_chain()
        
        return chains
    
    def _build_greeting_chain(self):
        """Build chain for greetings and simple interactions."""
        def greet(inputs):
            """Simple greeting response using OpenAI"""
            question = inputs.get("question", "")
            
            greeting_prompt = f"""Báº¡n lÃ  trá»£ lÃ½ AI thÃ¢n thiá»‡n vá» luáº­t giao thÃ´ng Viá»‡t Nam.
            
NgÆ°á»i dÃ¹ng: {question}
            
HÃ£y tráº£ lá»i Cá»°C Ká»² NGáº®N Gá»ŒN (1-2 cÃ¢u), thÃ¢n thiá»‡n. 
KHÃ”NG liá»‡t kÃª quy Ä‘á»‹nh.
KHÃ”NG Ä‘Æ°a ra "cÃ¢u há»i gá»£i Ã½".
CHá»ˆ chÃ o láº¡i hoáº·c cáº£m Æ¡n Ä‘Æ¡n giáº£n."""
            
            messages = [{"role": "user", "content": greeting_prompt}]
            return self._call_openai(messages, max_tokens=100)
        
        return RunnableLambda(greet)
    
    def _build_hipporag_chain(self):
        """
        Build HippoRAG chain - PRIMARY chain for all legal queries.
        Uses HippoRAG retriever (num_to_retrieve=60) + STRICT_QA_SYSTEM_PROMPT
        """
        def hipporag_qa(inputs):
            """HippoRAG-based QA with strict citation requirements"""
            question = inputs.get("question", "")
            
            print(f"ðŸ¦› HippoRAG Chain processing: {question[:50]}...")
            
            # Step 1: Retrieve documents using HippoRAG (retrieves 60 docs internally)
            try:
                docs = self.hipporag_retriever.invoke(question)
                print(f"   Retrieved {len(docs)} documents from HippoRAG")
            except Exception as e:
                print(f"âŒ HippoRAG retrieval failed: {e}")
                return f"Lá»—i truy xuáº¥t dá»¯ liá»‡u: {str(e)}"
            
            if not docs:
                return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong cÆ¡ sá»Ÿ dá»¯ liá»‡u vá» cÃ¢u há»i nÃ y."
            
            # Step 2: Format context from top 30 documents
            top_docs = docs[:30]
            context_parts = []
            for i, doc in enumerate(top_docs, 1):
                content = doc.page_content
                metadata = doc.metadata
                
                # Format citation from metadata
                citation = format_qdrant_citation(metadata)
                context_parts.append(f"[TÃ i liá»‡u {i}]\n{content}\n[Nguá»“n: {citation}]")
            
            context = "\n\n---\n\n".join(context_parts)
            
            USER_PROMPT_TEMPLATE = """
            CÃ‚U Há»ŽI:
            {question}

            CONTEXT (chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng thÃ´ng tin dÆ°á»›i Ä‘Ã¢y):
            {context}

            HÃ£y tráº£ lá»i Ä‘Ãºng theo cÃ¡c yÃªu cáº§u Ä‘Ã£ nÃªu trong SYSTEM PROMPT.
            """

            # Step 3: Generate answer using OpenAI with STRICT_QA_SYSTEM_PROMPT
            messages = [
                {"role": "system", "content": STRICT_QA_SYSTEM_PROMPT},
                {
        "role": "user",
        "content": USER_PROMPT_TEMPLATE.format(
            question=(question),
            context=context
        )
    }
            ]
            
            response = self._call_openai(messages, max_tokens=3000)
            print(f"âœ… HippoRAG Chain completed")
            
            return response
        
        return RunnableLambda(hipporag_qa)
    
    def _build_conversational_chain(self):
        """Build chain for conversational queries."""
        def conversational_response(inputs):
            question = inputs.get("question", "")
            previous_context = inputs.get("previous_context", "")
            
            conv_prompt = f"""Báº¡n lÃ  trá»£ lÃ½ AI thÃ¢n thiá»‡n chuyÃªn vá» luáº­t giao thÃ´ng Viá»‡t Nam.
            
Ngá»¯ cáº£nh trÆ°á»›c: {previous_context}
            
NgÆ°á»i dÃ¹ng: {question}
            
HÃ£y tráº£ lá»i tá»± nhiÃªn, thÃ¢n thiá»‡n nhÆ° cuá»™c trÃ² chuyá»‡n bÃ¬nh thÆ°á»ng. Chá»‰ tráº£ lá»i chung chung, ngáº¯n gá»n, khÃ´ng cáº§n trÃ­ch dáº«n Ä‘iá»u luáº­t."""
            
            messages = [{"role": "user", "content": conv_prompt}]
            return self._call_openai(messages, max_tokens=500)
        
        return RunnableLambda(conversational_response)
    
    def _build_web_search_chain(self):
        """Build chain for web search queries."""
        def web_search_response(inputs):
            question = inputs.get("question", "")
            
            try:
                from src.retrieval.web_search import get_web_search_tool
                web_tool = get_web_search_tool()
                results = web_tool.invoke(question)
                search_results = "\n".join([f"- {r.get('content', '')[:200]}..." for r in results[:3]])
            except Exception as e:
                search_results = f"Lá»—i tÃ¬m kiáº¿m: {e}"
            
            web_prompt = f"""Báº¡n lÃ  trá»£ lÃ½ AI tÃ¬m kiáº¿m thÃ´ng tin má»›i nháº¥t vá» luáº­t giao thÃ´ng Viá»‡t Nam.
            
Káº¿t quáº£ tÃ¬m kiáº¿m:
{search_results}
            
CÃ¢u há»i: {question}
            
HÃ£y:
- LIá»†T KÃŠ NGáº®N Gá»ŒN cÃ¡c quy Ä‘á»‹nh má»›i nháº¥t (dáº¡ng bullet points)
- TrÃ­ch dáº«n nguá»“n rÃµ rÃ ng
- KHÃ”NG giáº£i thÃ­ch chi tiáº¿t
- Chá»‰ thÃ´ng tin quan trá»ng nháº¥t"""
            
            messages = [{"role": "user", "content": web_prompt}]
            return self._call_openai(messages, max_tokens=1000)
        
        return RunnableLambda(web_search_response)
    
    def process_query(self, question: str, chat_history: list = None, user_id: str = None):
        """
        Process query using HippoRAG chain ONLY.
        No classification - all queries go through HippoRAG.
        """
        print(f"ðŸŸ¢ process_query: {question=}")
        
        try:
            # Check for safety
            if is_prompt_injection(question):
                return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ xá»­ lÃ½ yÃªu cáº§u nÃ y."
            
            if is_sensitive_content(question):
                return "Xin lá»—i, cÃ¢u há»i nÃ y náº±m ngoÃ i pháº¡m vi há»— trá»£ cá»§a tÃ´i."
            
            # ALWAYS use HippoRAG chain for all queries
            print("ðŸ¦› Using HippoRAG chain for query processing")
            response = self.chains[QueryType.SIMPLE_LEGAL].invoke({"question": question})
            
            print("âœ… Query processed successfully")
            return response
            
        except Exception as e:
            import traceback
            print("âŒ ERROR in process_query:")
            traceback.print_exc()
            return f"Xin lá»—i, cÃ³ lá»—i xáº£y ra: {e}"
    
    def clear_context(self, user_id: str):
        """Clear conversation context for user."""
        if self.conversation_context and user_id in self.conversation_context:
            del self.conversation_context[user_id]
    
    def get_conversation_stats(self) -> Dict[str, int]:
        """Get conversation statistics."""
        return {
            "total_sessions": len(self.conversation_context) if self.conversation_context else 0,
            "query_types_supported": len(QueryType)
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get chatbot system information."""
        return {
            "query_types": [qt.value for qt in QueryType],
            "active_chains": list(self.chains.keys()),
            "conversation_sessions": len(self.conversation_context) if self.conversation_context else 0,
            "primary_retriever": "HippoRAG",
            "memory_enabled": self.use_memory
        }
