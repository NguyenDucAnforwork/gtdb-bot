# src/chatbot_core.py
from typing import Dict, Any, List, Optional
from enum import Enum
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEmbeddings
from operator import itemgetter

# Core imports
from src.generation.openai_generator import get_llm
from src.retrieval.enhanced_retriever import (
    create_vector_only_retriever,
    create_hipporag_only_retriever, 
    format_qdrant_citation
)
from src.retrieval.query_transformer import create_query_transformer
from src.retrieval.reranker import create_reranker
from src.guardrails.injection_detector import is_prompt_injection
from src.guardrails.content_filter import is_sensitive_content
from src.routing import QueryRouter
from config import settings

# Query Types Enum for better classification
class QueryType(Enum):
    GREETING = "greeting"
    SIMPLE_LEGAL = "simple_legal"
    COMPLEX_LEGAL = "complex_legal"
    CONVERSATIONAL = "conversational"
    TECHNICAL = "technical"
    WEB_SEARCH = "web_search"


class ChatbotCore:
    def __init__(self):
        print("üöÄ Initializing Optimized Chatbot Core...")
        
        # Core models
        self.llm = get_llm()
        self.embeddings = HuggingFaceEmbeddings(model_name="AITeamVN/Vietnamese_Embedding")
        
        # Query classification
        self.query_classifier = self._build_query_classifier()
        
        # Memory answer validator
        self.memory_validator = self._build_memory_validator()
        
        # Initialize retrievers (selective initialization)
        print("üìö Initializing retrievers...")
        self.vector_retriever = create_vector_only_retriever(self.embeddings)
        self.hipporag_retriever = create_hipporag_only_retriever()
        
        # Default retriever based on query type
        self.current_retriever = self.vector_retriever
        
        # Build optimized chains for different query types
        self.chains = self._build_specialized_chains()
        
        # Simple memory for follow-ups (no complex caching)
        self.conversation_context = {}
        
        print("‚úÖ Chatbot Core initialized!")

    def _build_memory_validator(self):
        """Build LLM chain to validate if memory context can answer the question."""
        validator_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi.

NG·ªÆ C·∫¢NH T·ª™ B·ªò NH·ªö CU·ªòC TR√í CHUY·ªÜN:
{memory_context}

C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG: {question}

NHI·ªÜM V·ª§: ƒê√°nh gi√° xem ng·ªØ c·∫£nh tr√™n c√≥ ƒê·ª¶ TH√îNG TIN ƒë·ªÉ tr·∫£ l·ªùi C·ª§ TH·ªÇ v√† CH√çNH X√ÅC c√¢u h·ªèi kh√¥ng?

TI√äU CH√ç ƒê√ÅNH GI√Å:
‚úÖ C√ì ƒê·ª¶ TH√îNG TIN n·∫øu:
- Ng·ªØ c·∫£nh ch·ª©a TR·ª∞C TI·∫æP c√¢u tr·∫£ l·ªùi v·ªõi S·ªê LI·ªÜU C·ª§ TH·ªÇ (m·ª©c ph·∫°t, t·ªëc ƒë·ªô, v.v.)
- C√¢u h·ªèi l√† "t·ªïng c·ªông/c·ªông l·∫°i" v√† ng·ªØ c·∫£nh c√≥ ƒê·∫¶Y ƒê·ª¶ c√°c con s·ªë c·∫ßn t√≠nh
- C√¢u h·ªèi h·ªèi l·∫°i v·ªÅ ƒëi·ªÅu ƒë√£ ƒë∆∞·ª£c n√≥i tr∆∞·ªõc ƒë√≥ trong ng·ªØ c·∫£nh

‚ùå KH√îNG ƒê·ª¶ TH√îNG TIN n·∫øu:
- Ng·ªØ c·∫£nh ch·ªâ c√≥ c√¢u h·ªèi, KH√îNG c√≥ c√¢u tr·∫£ l·ªùi
- Ng·ªØ c·∫£nh tr·∫£ l·ªùi M∆† H·ªí, kh√¥ng c√≥ s·ªë li·ªáu c·ª• th·ªÉ
- C√¢u h·ªèi y√™u c·∫ßu th√¥ng tin M·ªöI ho√†n to√†n kh√¥ng c√≥ trong ng·ªØ c·∫£nh
- Ng·ªØ c·∫£nh c√≥ c√¢u tr·∫£ l·ªùi nh∆∞ng KH√îNG LI√äN QUAN ƒë·∫øn c√¢u h·ªèi hi·ªán t·∫°i

V√ç D·ª§:
1. Ng·ªØ c·∫£nh: "M·ª©c ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm l√† 400k-600k"
   C√¢u h·ªèi: "M·ª©c ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm l√† bao nhi√™u?"
   ‚Üí C√ì ƒê·ª¶ (tr·ª±c ti·∫øp tr·∫£ l·ªùi v·ªõi s·ªë c·ª• th·ªÉ)

2. Ng·ªØ c·∫£nh: "M·ª©c ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm?" (ch·ªâ l√† c√¢u h·ªèi)
   C√¢u h·ªèi: "M·ª©c ph·∫°t l√† bao nhi√™u?"
   ‚Üí KH√îNG ƒê·ª¶ (ng·ªØ c·∫£nh kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi)

3. Ng·ªØ c·∫£nh: "Ph·∫°t m≈©: 400k-600k\nPh·∫°t b·∫±ng l√°i: 4M-6M"
   C√¢u h·ªèi: "V·∫≠y t·ªïng c·ªông bao nhi√™u?"
   ‚Üí C√ì ƒê·ª¶ (c√≥ ƒë·ªß s·ªë li·ªáu ƒë·ªÉ t√≠nh t·ªïng)

4. Ng·ªØ c·∫£nh: "Ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm"
   C√¢u h·ªèi: "C√≤n n·∫øu kh√¥ng c√≥ b·∫±ng l√°i th√¨ sao?"
   ‚Üí KH√îNG ƒê·ª¶ (c√¢u h·ªèi v·ªÅ th√¥ng tin m·ªõi kh√¥ng c√≥ trong ng·ªØ c·∫£nh)

CH·ªà TR·∫¢ L·ªúI B·∫∞NG M·ªòT T·ª™: "YES" (c√≥ ƒë·ªß) ho·∫∑c "NO" (kh√¥ng ƒë·ªß)"""
        )
        
        return validator_prompt | self.llm | StrOutputParser()

    def _build_query_classifier(self):
        """Build smart query classifier."""
        classifier_prompt = ChatPromptTemplate.from_template(
            """Ph√¢n lo·∫°i c√¢u h·ªèi sau v√†o m·ªôt trong c√°c lo·∫°i:
            
1. GREETING: CH·ªà nh·ªØng c√¢u ch√†o h·ªèi thu·∫ßn t√∫y nh∆∞ "xin ch√†o", "hello", "hi", "c·∫£m ∆°n", "thank you". KH√îNG bao g·ªìm c√¢u c√≥ √Ω ƒë·ªãnh h·ªèi th√¥ng tin.
2. SIMPLE_LEGAL: H·ªèi ph√°p lu·∫≠t ƒë∆°n gi·∫£n, C·ª§ TH·ªÇ (v√≠ d·ª•: "m·ª©c ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm l√† bao nhi√™u?", "t·ªëc ƒë·ªô t·ªëi ƒëa trong khu d√¢n c∆∞?")
3. COMPLEX_LEGAL: H·ªèi ph√°p lu·∫≠t PH·ª®C T·∫†P (so s√°nh gi·ªØa c√°c nƒÉm, nhi·ªÅu ƒëi·ªÅu ki·ªán k·∫øt h·ª£p, ph√¢n t√≠ch s√¢u)
4. CONVERSATIONAL: Tr√≤ chuy·ªán chung chung, KH√îNG C·ª§ TH·ªÇ, KH√îNG h·ªèi v·ªÅ quy ƒë·ªãnh/m·ª©c ph·∫°t c·ª• th·ªÉ
5. WEB_SEARCH: H·ªèi v·ªÅ "quy ƒë·ªãnh M·ªöI NH·∫§T", "thay ƒë·ªïi g·∫ßn ƒë√¢y", "tin t·ª©c", "hi·ªán nay"

QUY T·∫ÆC QUAN TR·ªåNG:
- N·∫øu c√¢u h·ªèi c√≥ "m·ª©c ph·∫°t", "quy ƒë·ªãnh", "lu·∫≠t" + n·ªôi dung C·ª§ TH·ªÇ -> SIMPLE_LEGAL
- N·∫øu h·ªèi v·ªÅ "quy ƒë·ªãnh m·ªõi nh·∫•t", "thay ƒë·ªïi g√¨" NH∆ØNG KH√îNG c√≥ n·ªôi dung c·ª• th·ªÉ -> WEB_SEARCH
- "Xin ch√†o, t√¥i mu·ªën h·ªèi v·ªÅ X" -> CONVERSATIONAL (v√¨ ch·ªâ c√≥ √Ω ƒë·ªãnh h·ªèi chung chung)
- "T√¥i ƒëang quan t√¢m v·ªÅ quy ƒë·ªãnh m·ªõi nh·∫•t" -> WEB_SEARCH (v√¨ chung chung, c·∫ßn t√¨m ki·∫øm)

C√¢u h·ªèi: "{question}"

Ch·ªâ tr·∫£ l·ªùi b·∫±ng M·ªòT T·ª™: GREETING, SIMPLE_LEGAL, COMPLEX_LEGAL, CONVERSATIONAL, WEB_SEARCH"""
        )
        
        return classifier_prompt | self.llm | StrOutputParser()
    
    def _build_specialized_chains(self):
        """Build specialized chains for different query types."""
        chains = {}
        
        # Greeting chain (no retrieval needed)
        chains[QueryType.GREETING] = self._build_greeting_chain()
        
        # Simple legal chain (MEMORY FIRST -> vector search -> web search -> hipporag)
        chains[QueryType.SIMPLE_LEGAL] = self._build_simple_legal_chain()
        
        # Complex legal chain (HippoRAG + vector)
        chains[QueryType.COMPLEX_LEGAL] = self._build_complex_legal_chain()
        
        # Conversational chain (minimal retrieval)
        chains[QueryType.CONVERSATIONAL] = self._build_conversational_chain()
        
        # Web search chain (external search)
        chains[QueryType.WEB_SEARCH] = self._build_web_search_chain()
        
        return chains
    
    def _build_greeting_chain(self):
        """Build chain for greetings and simple interactions."""
        greeting_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† tr·ª£ l√Ω AI th√¢n thi·ªán v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam.
            
Ng∆∞·ªùi d√πng: {question}
            
H√£y tr·∫£ l·ªùi C·ª∞C K·ª≤ NG·∫ÆN G·ªåN (1-2 c√¢u), th√¢n thi·ªán. 
KH√îNG li·ªát k√™ quy ƒë·ªãnh.
KH√îNG ƒë∆∞a ra "c√¢u h·ªèi g·ª£i √Ω".
CH·ªà ch√†o l·∫°i ho·∫∑c c·∫£m ∆°n ƒë∆°n gi·∫£n."""
        )
        
        return greeting_prompt | self.llm | StrOutputParser()
    
    def _build_simple_legal_chain(self):
        """Build chain with MEMORY as FIRST priority retriever."""
        simple_legal_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† chuy√™n gia lu·∫≠t giao th√¥ng Vi·ªát Nam. Tr·∫£ l·ªùi CH√çNH X√ÅC d·ª±a tr√™n th√¥ng tin c√≥ s·∫µn.
            
TH√îNG TIN:
{context}
            
C√ÇU H·ªéI: {question}
            
H√ÉY TR·∫¢ L·ªúI:
- N·∫øu th√¥ng tin ƒë·ªß: ƒê∆∞a ra c√¢u tr·∫£ l·ªùi C·ª§ TH·ªÇ v√† R√ï R√ÄNG (s·ªë ti·ªÅn ph·∫°t, t·ªëc ƒë·ªô, v.v.)
- N·∫øu c·∫ßn T√çNH TO√ÅN (nh∆∞ "t·ªïng c·ªông"): H√£y T√çNH TO√ÅN v√† ƒë∆∞a ra k·∫øt qu·∫£ c·ª• th·ªÉ
- Tr√≠ch d·∫´n ngu·ªìn ph√°p l√Ω n·∫øu c√≥ (Ngh·ªã ƒë·ªãnh, ƒêi·ªÅu, Kho·∫£n)
- Gi·ªçng nh∆∞ cu·ªôc tr√≤ chuy·ªán t·ª± nhi√™n, NG·∫ÆN G·ªåN

L∆ØU √ù: N·∫øu th√¥ng tin KH√îNG ƒë·ªß, h√£y n√≥i th·∫≥ng "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ v·ªÅ {question} trong c∆° s·ªü d·ªØ li·ªáu."""
        )
        
        def smart_retrieval_with_memory_first(inputs):
            """MEMORY FIRST retrieval strategy with strict validation."""
            question = inputs["question"]
            memory_context = inputs.get("memory_context", "")
            
            # ‚úÖ STEP 0: Try MEMORY FIRST (highest priority)
            if memory_context and memory_context.strip():
                print("üß† Checking if MEMORY can answer the question...")
                
                try:
                    # Use LLM to validate if memory has enough info
                    validation_result = self.memory_validator.invoke({
                        "memory_context": memory_context,
                        "question": question
                    }).strip().upper()
                    
                    if validation_result == "YES":
                        print("‚úÖ MEMORY has sufficient information! Using memory directly.")
                        
                        # Generate answer from memory using LLM
                        memory_answer_prompt = ChatPromptTemplate.from_template(
                            """D·ª±a v√†o NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch T·ª∞ NHI√äN v√† C·ª§ TH·ªÇ.

NG·ªÆ C·∫¢NH:
{memory_context}

C√ÇU H·ªéI: {question}

Y√äU C·∫¶U:
- N·∫øu c·∫ßn T√çNH TO√ÅN (nh∆∞ "t·ªïng c·ªông"), h√£y T√çNH v√† ƒë∆∞a ra K·∫æT QU·∫¢ C·ª§ TH·ªÇ
- Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, gi·ªçng ƒëi·ªáu T·ª∞ NHI√äN nh∆∞ ƒëang tr√≤ chuy·ªán
- KH√îNG c·∫ßn tr√≠ch d·∫´n ngu·ªìn v√¨ ƒë√¢y l√† th√¥ng tin t·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc
- CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong ng·ªØ c·∫£nh

TR·∫¢ L·ªúI:"""
                        )
                        
                        answer_chain = memory_answer_prompt | self.llm | StrOutputParser()
                        memory_answer = answer_chain.invoke({
                            "memory_context": memory_context,
                            "question": question
                        })
                        
                        return f"[T·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc]\n{memory_answer}"
                    else:
                        print("‚ö†Ô∏è MEMORY validation failed - information insufficient or irrelevant")
                        print(f"   Validation result: {validation_result}")
                        
                except Exception as e:
                    print(f"‚ùå Memory validation error: {e}")
            else:
                print("‚ÑπÔ∏è No memory context available, skipping memory retrieval")
            
            # Helper function to check if response is insufficient
            def is_insufficient_response(response_text):
                insufficient_indicators = [
                    "kh√¥ng th·ªÉ x√°c ƒë·ªãnh", "kh√¥ng c√≥ th√¥ng tin", "kh√¥ng t√¨m th·∫•y",
                    "d·ª±a tr√™n t√†i li·ªáu", "do kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu",
                    "t√¥i kh√¥ng th·ªÉ tr√≠ch d·∫´n", "kh√¥ng th·ªÉ n√™u r√µ", "kh√¥ng c√≥ ƒëi·ªÅu kho·∫£n", "r·∫•t ti·∫øc",
                    "b·∫°n c·∫ßn tham kh·∫£o", "kh√¥ng ƒë·ªÅ c·∫≠p", "th√¥ng tin b·∫°n cung c·∫•p kh√¥ng",
                    "c√°c vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t kh√°c", "ch·ªâ quy ƒë·ªãnh chung"
                ]
                response_lower = response_text.lower()
                
                has_indicator = any(indicator in response_lower for indicator in insufficient_indicators)
                has_specific_info = any(char.isdigit() for char in response_text)
                
                return has_indicator or not has_specific_info
            
            # ‚úÖ STEP 1: Try Vector retriever
            try:
                query_transformer = create_query_transformer(self.vector_retriever, self.llm)
                reranker = create_reranker(query_transformer)
                vector_docs = reranker.invoke(question)
                
                if vector_docs and len(vector_docs) > 0:
                    print("üìö Trying vector retriever")
                    vector_context = []
                    for doc in vector_docs[:3]:
                        content = doc.page_content
                        metadata = doc.metadata
                        citation = format_qdrant_citation(metadata)
                        vector_context.append(f"{content}\n[Ngu·ªìn: {citation}]")
                    
                    vector_formatted = "\n\n".join(vector_context)
                    
                    test_response = self.llm.invoke(
                        simple_legal_prompt.format(context=vector_formatted, question=question)
                    )
                    
                    response_text = test_response.content if hasattr(test_response, 'content') else str(test_response)
                    
                    if not is_insufficient_response(response_text):
                        print("‚úÖ Vector retriever provided sufficient answer")
                        return vector_formatted
                    else:
                        print("‚ö†Ô∏è Vector answer insufficient, trying web search")
            except Exception as e:
                print(f"‚ùå Vector retriever error: {e}")
            
            # ‚úÖ STEP 2: Try Web search
            try:
                print("üåê Falling back to web search")
                from src.retrieval.web_search import get_web_search_tool
                web_tool = get_web_search_tool()
                web_results = web_tool.invoke(question)
                
                if web_results:
                    web_content = "\n".join([f"- {r.get('content', '')[:300]}" for r in web_results[:3]])
                    web_context = f"Th√¥ng tin t√¨m ki·∫øm tr√™n web:\n{web_content}"
                    
                    test_response = self.llm.invoke(
                        simple_legal_prompt.format(context=web_context, question=question)
                    )
                    
                    response_text = test_response.content if hasattr(test_response, 'content') else str(test_response)
                    
                    if not is_insufficient_response(response_text):
                        print("‚úÖ Web search provided sufficient answer")
                        return web_context
                    else:
                        print("‚ö†Ô∏è Web search answer insufficient, trying HippoRAG")
            except Exception as e:
                print(f"‚ùå Web search error: {e}")
            
            # ‚úÖ STEP 3: Try HippoRAG as last resort
            try:
                print("ü¶Ñ Falling back to HippoRAG")
                hippo_docs = self.hipporag_retriever.invoke(question)
                if hippo_docs:
                    hippo_context = "\n".join([doc.page_content for doc in hippo_docs[:2]])
                    print("‚úÖ Using HippoRAG as final fallback")
                    return hippo_context
            except Exception as e:
                print(f"‚ùå HippoRAG error: {e}")
            
            return "Xin l·ªói, kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c v·ªÅ c√¢u h·ªèi n√†y."
        
        return (
            {
                "question": itemgetter("question"),
                "memory_context": itemgetter("memory_context"),  # Pass memory from process_query
                "context": RunnableLambda(smart_retrieval_with_memory_first)
            }
            | simple_legal_prompt
            | self.llm
            | StrOutputParser()
        )
    
    def _build_complex_legal_chain(self):
        """Build chain for complex legal queries using HippoRAG + vector."""
        complex_legal_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† chuy√™n gia lu·∫≠t cao c·∫•p, gi·ªèi ph√¢n t√≠ch ph·ª©c t·∫°p v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam.
            
TH√îNG TIN T·ª™ KI·∫æN TH·ª®C GRAPH:
{hippo_context}
            
TH√îNG TIN T·ª™ T√ÄI LI·ªÜU:
{vector_context}
            
C√ÇU H·ªéI PH·ª®C T·∫†P: {question}
            
H√ÉY:
1. K·∫æT H·ª¢P th√¥ng tin t·ª´ c·∫£ hai ngu·ªìn
2. PH√ÇN T√çCH so s√°nh, li√™n k·∫øt
3. ƒê∆ØA RA k·∫øt lu·∫≠n r√µ r√†ng
4. TR√çCH D·∫™N ƒë·∫ßy ƒë·ªß cƒÉn c·ª© ph√°p l√Ω
            
TR·∫¢ L·ªúI CHI TI·∫æT:"""
        )
        
        vector_transformer = create_query_transformer(self.vector_retriever, self.llm)
        vector_reranker = create_reranker(vector_transformer)
        
        def format_contexts(inputs):
            question = inputs["question"]
            
            vector_docs = vector_reranker.invoke(question)
            vector_context = "\n".join([doc.page_content for doc in vector_docs[:3]])
            
            try:
                hippo_docs = self.hipporag_retriever.invoke(question)
                hippo_context = "\n".join([doc.page_content for doc in hippo_docs[:3]])
            except Exception as e:
                print(f"HippoRAG error: {e}")
                hippo_context = "Kh√¥ng c√≥ th√¥ng tin t·ª´ knowledge graph"
            
            return {
                "question": question,
                "vector_context": vector_context,
                "hippo_context": hippo_context
            }
        
        return (
            RunnableLambda(format_contexts)
            | complex_legal_prompt
            | self.llm
            | StrOutputParser()
        )
    
    def _build_conversational_chain(self):
        """Build chain for conversational queries."""
        conv_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† tr·ª£ l√Ω AI th√¢n thi·ªán chuy√™n v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam.
            
Ng·ªØ c·∫£nh tr∆∞·ªõc: {previous_context}
            
Ng∆∞·ªùi d√πng: {question}
            
H√£y tr·∫£ l·ªùi t·ª± nhi√™n, th√¢n thi·ªán nh∆∞ cu·ªôc tr√≤ chuy·ªán b√¨nh th∆∞·ªùng. Ch·ªâ tr·∫£ l·ªùi chung chung, ng·∫Øn g·ªçn, kh√¥ng c·∫ßn tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t."""
        )
        
        return conv_prompt | self.llm | StrOutputParser()
    
    def _build_web_search_chain(self):
        """Build chain for web search queries."""
        web_prompt = ChatPromptTemplate.from_template(
            """B·∫°n l√† tr·ª£ l√Ω AI t√¨m ki·∫øm th√¥ng tin m·ªõi nh·∫•t v·ªÅ lu·∫≠t giao th√¥ng Vi·ªát Nam.
            
K·∫øt qu·∫£ t√¨m ki·∫øm:
{search_results}
            
C√¢u h·ªèi: {question}
            
H√£y:
- LI·ªÜT K√ä NG·∫ÆN G·ªåN c√°c quy ƒë·ªãnh m·ªõi nh·∫•t (d·∫°ng bullet points)
- Tr√≠ch d·∫´n ngu·ªìn r√µ r√†ng
- KH√îNG gi·∫£i th√≠ch chi ti·∫øt
- Ch·ªâ th√¥ng tin quan tr·ªçng nh·∫•t"""
        )
        
        def web_search_and_format(inputs):
            question = inputs["question"]
            try:
                from src.retrieval.web_search import get_web_search_tool
                web_tool = get_web_search_tool()
                results = web_tool.invoke(question)
                search_results = "\n".join([f"- {r.get('content', '')[:200]}..." for r in results[:3]])
                return {"question": question, "search_results": search_results}
            except Exception as e:
                return {"question": question, "search_results": f"L·ªói t√¨m ki·∫øm: {e}"}
        
        return RunnableLambda(web_search_and_format) | web_prompt | self.llm | StrOutputParser()
    
    def process_query(self, question: str, chat_history: list = None, user_id: str = None):
        """Enhanced query processing with MEMORY as first-priority retriever."""
        print(f"üü¢ process_query: {question=}, {len(chat_history or [])=}")
        
        try:
            # Step 1: Format memory context
            memory_context = ""
            if chat_history and len(chat_history) > 0:
                # Use last 5 memories as context
                recent_memories = chat_history[-5:] if len(chat_history) > 5 else chat_history
                memory_context = "\n".join([f"- {mem}" for mem in recent_memories])
                print(f"üß† Prepared {len(recent_memories)} memories as context")
            
            # Step 2: Classify query
            query_type = self._classify_query(question, chat_history or [])
            print(f"üéØ Query classified as: {query_type.value}")
            
            # Step 3: Process with appropriate chain
            if query_type in self.chains:
                if query_type == QueryType.CONVERSATIONAL:
                    response = self.chains[query_type].invoke({
                        "question": question,
                        "previous_context": "\n".join(chat_history[-3:]) if chat_history else ""
                    })
                elif query_type == QueryType.SIMPLE_LEGAL:
                    # ‚úÖ Pass memory_context to simple legal chain
                    response = self.chains[query_type].invoke({
                        "question": question,
                        "memory_context": memory_context
                    })
                else:
                    response = self.chains[query_type].invoke({"question": question})
            else:
                response = "Xin l·ªói, t√¥i kh√¥ng hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n."
            
            # Step 4: Add follow-up suggestions
            final_response = self._create_response(response, query_type, question)
            
            print("‚úÖ Query processed successfully")
            return final_response
            
        except Exception as e:
            import traceback
            print("‚ùå ERROR in process_query:")
            traceback.print_exc()
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {e}"
    
    def _classify_query(self, question: str, chat_history: list) -> QueryType:
        """Classify query using LLM."""
        try:
            classification = self.query_classifier.invoke({"question": question})
            classification = classification.strip().upper()
            
            type_mapping = {
                "GREETING": QueryType.GREETING,
                "SIMPLE_LEGAL": QueryType.SIMPLE_LEGAL,
                "COMPLEX_LEGAL": QueryType.COMPLEX_LEGAL,
                "CONVERSATIONAL": QueryType.CONVERSATIONAL,
                "TECHNICAL": QueryType.CONVERSATIONAL,
                "WEB_SEARCH": QueryType.WEB_SEARCH
            }
            
            return type_mapping.get(classification, QueryType.CONVERSATIONAL)
        except Exception as e:
            print(f"‚ö†Ô∏è Classification error: {e}")
            return QueryType.CONVERSATIONAL
    
    def _create_response(self, content: str, query_type: QueryType, original_question: str = "") -> str:
        """Create formatted response with natural follow-up suggestions."""
        # Skip follow-ups if answer came from memory (starts with "[T·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc]")
        if content.startswith("[T·ª´ cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc]"):
            return content
        
        if query_type in [QueryType.SIMPLE_LEGAL, QueryType.COMPLEX_LEGAL]:
            has_suggestions = any(phrase in content.lower() for phrase in ['c√¢u h·ªèi g·ª£i √Ω', 'follow-up', 'b·∫°n c√≥ th·ªÉ', 'n·∫øu b·∫°n'])
            is_insufficient = any(phrase in content.lower() for phrase in ['kh√¥ng t√¨m th·∫•y', 'kh√¥ng c√≥ th√¥ng tin', 'xin l·ªói'])
            is_too_short = len(content) < 200
            
            if not has_suggestions and not is_insufficient and not is_too_short:
                follow_ups = self._generate_follow_ups(query_type, original_question)
                if follow_ups:
                    return f"{content}\n\n{follow_ups}"
        return content
    
    def _generate_follow_ups(self, query_type: QueryType, original_question: str = "") -> str:
        """Generate natural follow-up suggestions using LLM."""
        followup_prompt = ChatPromptTemplate.from_template(
            """D·ª±a tr√™n c√¢u h·ªèi g·ªëc, h√£y t·∫°o th√™m m·ªôt g·ª£i √Ω ti·∫øp theo t·ª± nhi√™n v√† h·∫•p d·∫´n.
            
C√¢u h·ªèi g·ªëc: "{original_question}"
Lo·∫°i: {query_type}
            
T·∫°o c√°c c√¢u g·ª£i √Ω nh∆∞:
- "N·∫øu b·∫°n mu·ªën bi·∫øt th√™m v·ªÅ X, t√¥i c√≥ th·ªÉ gi·∫£i th√≠ch chi ti·∫øt h∆°n"
- "B·∫°n c≈©ng c√≥ th·ªÉ h·ªèi v·ªÅ Y n·∫øu quan t√¢m" 
- "T√¥i c≈©ng c√≥ th·ªÉ h∆∞·ªõng d·∫´n v·ªÅ Z n·∫øu b·∫°n c·∫ßn"
            
H√£y vi·∫øt t·ª± nhi√™n, th√¢n thi·ªán."""
        )
        
        try:
            followup_chain = followup_prompt | self.llm | StrOutputParser()
            follow_ups = followup_chain.invoke({
                "query_type": query_type.value,
                "original_question": original_question
            })
            return follow_ups.strip()
        except Exception as e:
            print(f"‚ö†Ô∏è Follow-up generation error: {e}")
            return "- N·∫øu b·∫°n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c v·ªÅ lu·∫≠t giao th√¥ng, t√¥i s·∫µn s√†ng gi·∫£i ƒë√°p!"
    
    def clear_context(self, user_id: str):
        """Clear conversation context for user."""
        if user_id in self.conversation_context:
            del self.conversation_context[user_id]
    
    def get_conversation_stats(self) -> Dict[str, int]:
        """Get conversation statistics."""
        return {
            "total_sessions": len(self.conversation_context),
            "query_types_supported": len(QueryType)
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get chatbot system information."""
        return {
            "query_types": [qt.value for qt in QueryType],
            "active_chains": list(self.chains.keys()),
            "conversation_sessions": len(self.conversation_context),
            "retrievers": {
                "memory": "Priority 0 - Conversation history",
                "vector": "Priority 1 - Qdrant vector store",
                "web": "Priority 2 - Web search",
                "hipporag": "Priority 3 - Knowledge graph"
            }
        }