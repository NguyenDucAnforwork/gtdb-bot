# src/chatbot_core.py
from typing import Dict, Any, List, Optional
from enum import Enum
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEmbeddings
from operator import itemgetter

# Core imports
from src.generation.openai_generator import get_llm
from src.retrieval.enhanced_retriever import (
    create_vector_only_retriever,
    create_hipporag_only_retriever, 
    format_qdrant_citation
)
from src.retrieval.vietnamese_law_prompts import get_vietnamese_law_prompts
from src.guardrails.injection_detector import is_prompt_injection
from src.guardrails.content_filter import is_sensitive_content
from config import settings

# Query Types Enum for better classification
class QueryType(Enum):
    GREETING = "greeting"
    SIMPLE_LEGAL = "simple_legal"
    COMPLEX_LEGAL = "complex_legal"
    CONVERSATIONAL = "conversational"
    TECHNICAL = "technical"
    WEB_SEARCH = "web_search"


# STRICT_QA_SYSTEM_PROMPT - 5-section structured response format with citation enforcement
STRICT_QA_SYSTEM_PROMPT = """Báº¡n lÃ  trá»£ lÃ½ phÃ¡p luáº­t chuyÃªn vá» luáº­t giao thÃ´ng Viá»‡t Nam. HÃ£y tráº£ lá»i cÃ¢u há»i CHá»ˆ Dá»°A TRÃŠN cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.

QUY Táº®C Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p
2. PHáº¢I trÃ­ch dáº«n nguá»“n cá»¥ thá»ƒ (Nghá»‹ Ä‘á»‹nh, Äiá»u, Khoáº£n, Äiá»ƒm)
3. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong tÃ i liá»‡u, nÃ³i rÃµ "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong cÆ¡ sá»Ÿ dá»¯ liá»‡u"
4. KHÃ”NG bá»‹a ra thÃ´ng tin hoáº·c dÃ¹ng kiáº¿n thá»©c bÃªn ngoÃ i

Äá»ŠNH Dáº NG TRáº¢ Lá»œI (5 PHáº¦N):

## 1. TÃ“M Táº®T NHANH
[1-2 cÃ¢u tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i]

## 2. CHI TIáº¾T QUY Äá»ŠNH
[Liá»‡t kÃª cÃ¡c quy Ä‘á»‹nh liÃªn quan vá»›i má»©c pháº¡t/Ä‘iá»u kiá»‡n cá»¥ thá»ƒ]

## 3. TRÃCH DáºªN NGUá»’N
[Ghi rÃµ: Nghá»‹ Ä‘á»‹nh sá»‘..., Äiá»u..., Khoáº£n..., Äiá»ƒm...]

## 4. LÆ¯U Ã QUAN TRá»ŒNG
[CÃ¡c trÆ°á»ng há»£p Ä‘áº·c biá»‡t hoáº·c ngoáº¡i lá»‡ náº¿u cÃ³]

## 5. CÃ‚U Há»ŽI LIÃŠN QUAN
[1-2 cÃ¢u há»i gá»£i Ã½ ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ quan tÃ¢m]
"""


class ChatbotCore:
    def __init__(self, use_memory: bool = False):
        """
        Initialize Optimized Chatbot Core
        
        Args:
            use_memory: Náº¿u True, sá»­ dá»¥ng memory Ä‘á»ƒ lÆ°u vÃ  truy xuáº¥t conversation history.
                       Náº¿u False, táº¯t táº¥t cáº£ memory functionality (default: False)
        """
        print("ðŸš€ Initializing Optimized Chatbot Core...")
        print(f"   Memory: {'ENABLED' if use_memory else 'DISABLED'}")
        
        # Store memory flag
        self.use_memory = use_memory
        
        # Core models - OpenAI client (not langchain)
        self.openai_client = get_llm()
        self.embeddings = HuggingFaceEmbeddings(model_name="AITeamVN/Vietnamese_Embedding")
        
        # Initialize retrievers (selective initialization)
        print("ðŸ“š Initializing retrievers...")
        self.vector_retriever = create_vector_only_retriever(self.embeddings)
        self.hipporag_retriever = create_hipporag_only_retriever()
        
        # Default retriever based on query type
        self.current_retriever = self.vector_retriever
        
        # Build optimized chains for different query types
        self.chains = self._build_specialized_chains()
        
        # Simple memory for follow-ups (only if memory is enabled)
        if self.use_memory:
            self.conversation_context = {}
        else:
            self.conversation_context = None
        
        print("âœ… Chatbot Core initialized!")
    
    def _call_openai(self, messages: list, max_tokens: int = 3000) -> str:
        """
        Helper method to call OpenAI API directly
        Replacement for langchain's LLM invocation
        """
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_completion_tokens=max_tokens,
                temperature=0
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"âŒ OpenAI API call failed: {e}")
            return f"Lá»—i khi gá»i API: {str(e)}"
    
    def _build_specialized_chains(self):
        """Build specialized chains for different query types."""
        chains = {}
        
        # Greeting chain (no retrieval needed)
        chains[QueryType.GREETING] = self._build_greeting_chain()
        
        # HippoRAG chain - PRIMARY chain cho ALL legal queries (retrieve 60 docs + strict prompt)
        hipporag_chain = self._build_hipporag_chain()
        chains[QueryType.SIMPLE_LEGAL] = hipporag_chain
        chains[QueryType.COMPLEX_LEGAL] = hipporag_chain
        
        # Conversational chain (minimal retrieval)
        chains[QueryType.CONVERSATIONAL] = self._build_conversational_chain()
        
        # Web search chain (external search)
        chains[QueryType.WEB_SEARCH] = self._build_web_search_chain()
        
        return chains
    
    def _build_greeting_chain(self):
        """Build chain for greetings and simple interactions."""
        def greet(inputs):
            """Simple greeting response using OpenAI"""
            question = inputs.get("question", "")
            
            greeting_prompt = f"""Báº¡n lÃ  trá»£ lÃ½ AI thÃ¢n thiá»‡n vá» luáº­t giao thÃ´ng Viá»‡t Nam.
            
NgÆ°á»i dÃ¹ng: {question}
            
HÃ£y tráº£ lá»i Cá»°C Ká»² NGáº®N Gá»ŒN (1-2 cÃ¢u), thÃ¢n thiá»‡n. 
KHÃ”NG liá»‡t kÃª quy Ä‘á»‹nh.
KHÃ”NG Ä‘Æ°a ra "cÃ¢u há»i gá»£i Ã½".
CHá»ˆ chÃ o láº¡i hoáº·c cáº£m Æ¡n Ä‘Æ¡n giáº£n."""
            
            messages = [{"role": "user", "content": greeting_prompt}]
            return self._call_openai(messages, max_tokens=100)
        
        return RunnableLambda(greet)
    
    def _build_hipporag_chain(self):
        """
        Build HippoRAG chain - PRIMARY chain for all legal queries.
        Uses HippoRAG retriever (num_to_retrieve=60) + STRICT_QA_SYSTEM_PROMPT
        """
        def hipporag_qa(inputs):
            """HippoRAG-based QA with strict citation requirements"""
            question = inputs.get("question", "")
            
            print(f"ðŸ¦› HippoRAG Chain processing: {question[:50]}...")
            
            # Step 1: Retrieve documents using HippoRAG (retrieves 60 docs internally)
            try:
                docs = self.hipporag_retriever.invoke(question)
                print(f"   Retrieved {len(docs)} documents from HippoRAG")
            except Exception as e:
                print(f"âŒ HippoRAG retrieval failed: {e}")
                return f"Lá»—i truy xuáº¥t dá»¯ liá»‡u: {str(e)}"
            
            if not docs:
                return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong cÆ¡ sá»Ÿ dá»¯ liá»‡u vá» cÃ¢u há»i nÃ y."
            
            # Step 2: Format context from top 30 documents
            top_docs = docs[:30]
            context_parts = []
            for i, doc in enumerate(top_docs, 1):
                content = doc.page_content
                metadata = doc.metadata
                
                # Format citation from metadata
                citation = format_qdrant_citation(metadata)
                context_parts.append(f"[TÃ i liá»‡u {i}]\n{content}\n[Nguá»“n: {citation}]")
            
            context = "\n\n---\n\n".join(context_parts)
            
            # Step 3: Generate answer using OpenAI with STRICT_QA_SYSTEM_PROMPT
            messages = [
                {"role": "system", "content": STRICT_QA_SYSTEM_PROMPT},
                {"role": "user", "content": f"""TÃ€I LIá»†U THAM KHáº¢O:
{context}

CÃ‚U Há»ŽI: {question}

HÃ£y tráº£ lá»i theo Ä‘Ãºng Ä‘á»‹nh dáº¡ng 5 pháº§n Ä‘Ã£ quy Ä‘á»‹nh."""}
            ]
            
            response = self._call_openai(messages, max_tokens=3000)
            print(f"âœ… HippoRAG Chain completed")
            
            return response
        
        return RunnableLambda(hipporag_qa)
    
    def _build_conversational_chain(self):
        """Build chain for conversational queries."""
        def conversational_response(inputs):
            question = inputs.get("question", "")
            previous_context = inputs.get("previous_context", "")
            
            conv_prompt = f"""Báº¡n lÃ  trá»£ lÃ½ AI thÃ¢n thiá»‡n chuyÃªn vá» luáº­t giao thÃ´ng Viá»‡t Nam.
            
Ngá»¯ cáº£nh trÆ°á»›c: {previous_context}
            
NgÆ°á»i dÃ¹ng: {question}
            
HÃ£y tráº£ lá»i tá»± nhiÃªn, thÃ¢n thiá»‡n nhÆ° cuá»™c trÃ² chuyá»‡n bÃ¬nh thÆ°á»ng. Chá»‰ tráº£ lá»i chung chung, ngáº¯n gá»n, khÃ´ng cáº§n trÃ­ch dáº«n Ä‘iá»u luáº­t."""
            
            messages = [{"role": "user", "content": conv_prompt}]
            return self._call_openai(messages, max_tokens=500)
        
        return RunnableLambda(conversational_response)
    
    def _build_web_search_chain(self):
        """Build chain for web search queries."""
        def web_search_response(inputs):
            question = inputs.get("question", "")
            
            try:
                from src.retrieval.web_search import get_web_search_tool
                web_tool = get_web_search_tool()
                results = web_tool.invoke(question)
                search_results = "\n".join([f"- {r.get('content', '')[:200]}..." for r in results[:3]])
            except Exception as e:
                search_results = f"Lá»—i tÃ¬m kiáº¿m: {e}"
            
            web_prompt = f"""Báº¡n lÃ  trá»£ lÃ½ AI tÃ¬m kiáº¿m thÃ´ng tin má»›i nháº¥t vá» luáº­t giao thÃ´ng Viá»‡t Nam.
            
Káº¿t quáº£ tÃ¬m kiáº¿m:
{search_results}
            
CÃ¢u há»i: {question}
            
HÃ£y:
- LIá»†T KÃŠ NGáº®N Gá»ŒN cÃ¡c quy Ä‘á»‹nh má»›i nháº¥t (dáº¡ng bullet points)
- TrÃ­ch dáº«n nguá»“n rÃµ rÃ ng
- KHÃ”NG giáº£i thÃ­ch chi tiáº¿t
- Chá»‰ thÃ´ng tin quan trá»ng nháº¥t"""
            
            messages = [{"role": "user", "content": web_prompt}]
            return self._call_openai(messages, max_tokens=1000)
        
        return RunnableLambda(web_search_response)
    
    def process_query(self, question: str, chat_history: list = None, user_id: str = None):
        """
        Process query using HippoRAG chain ONLY.
        No classification - all queries go through HippoRAG.
        """
        print(f"ðŸŸ¢ process_query: {question=}")
        
        try:
            # Check for safety
            if is_prompt_injection(question):
                return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ xá»­ lÃ½ yÃªu cáº§u nÃ y."
            
            if is_sensitive_content(question):
                return "Xin lá»—i, cÃ¢u há»i nÃ y náº±m ngoÃ i pháº¡m vi há»— trá»£ cá»§a tÃ´i."
            
            # ALWAYS use HippoRAG chain for all queries
            print("ðŸ¦› Using HippoRAG chain for query processing")
            response = self.chains[QueryType.SIMPLE_LEGAL].invoke({"question": question})
            
            print("âœ… Query processed successfully")
            return response
            
        except Exception as e:
            import traceback
            print("âŒ ERROR in process_query:")
            traceback.print_exc()
            return f"Xin lá»—i, cÃ³ lá»—i xáº£y ra: {e}"
    
    def clear_context(self, user_id: str):
        """Clear conversation context for user."""
        if self.conversation_context and user_id in self.conversation_context:
            del self.conversation_context[user_id]
    
    def get_conversation_stats(self) -> Dict[str, int]:
        """Get conversation statistics."""
        return {
            "total_sessions": len(self.conversation_context) if self.conversation_context else 0,
            "query_types_supported": len(QueryType)
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get chatbot system information."""
        return {
            "query_types": [qt.value for qt in QueryType],
            "active_chains": list(self.chains.keys()),
            "conversation_sessions": len(self.conversation_context) if self.conversation_context else 0,
            "primary_retriever": "HippoRAG",
            "memory_enabled": self.use_memory
        }
