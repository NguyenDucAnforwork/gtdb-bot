# src/ingestion/updater.py
import os
import re
import json
import uuid
import pathlib
import subprocess
from typing import List
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
from hipporag import HippoRAG

# Load biáº¿n mÃ´i trÆ°á»ng
load_dotenv()

# Cáº¥u hÃ¬nh Regex (Láº¥y tá»« notebooks/HippoRAG.ipynb)
RX_DIEU = re.compile(r'(?m)^\s*Äiá»u\s+(\d+)\s*[.:]', re.UNICODE)
RX_KHOAN_NUM = re.compile(r'(?m)\s*(\d+)\.\s', re.UNICODE)
RX_KHOAN_WORD = re.compile(r'(?m)\s*Khoáº£n\s+(\d+)\s*[.:]\s*', re.UNICODE)
RX_DIEM_LETTER = re.compile(r'(?m)\s*([a-z])\)\s', re.UNICODE)

def extract_text_from_pdf(pdf_path: str) -> str:
    """DÃ¹ng pdftotext Ä‘á»ƒ láº¥y text tá»« PDF (giá»¯ layout tá»‘t hÆ¡n pypdf)"""
    txt_path = pdf_path + ".txt"
    try:
        # YÃªu cáº§u há»‡ thá»‘ng Ä‘Ã£ cÃ i poppler-utils
        subprocess.run(["pdftotext", "-layout", pdf_path, txt_path], check=True)
        text = pathlib.Path(txt_path).read_text(encoding="utf-8", errors="ignore")
    except Exception as e:
        print(f"Lá»—i khi Ä‘á»c PDF: {e}")
        return ""
    finally:
        if os.path.exists(txt_path):
            os.remove(txt_path)
    return text

def _slice_by_matches(text: str, matches: List[re.Match]) -> List[tuple]:
    if not matches: return []
    spans = [(m.start(), m) for m in matches]
    spans.append((len(text), None))
    out = []
    for i in range(len(spans)-1):
        start, m = spans[i]
        end, _ = spans[i+1]
        out.append((start, end, m))
    return out

def split_passages(raw: str, law_code: str) -> List[str]:
    """Logic tÃ¡ch vÄƒn báº£n theo Äiá»u -> Khoáº£n (tá»« Notebook)"""
    passages = []
    # 1. TÃ¡ch theo Äiá»u
    dieu_matches = list(RX_DIEU.finditer(raw))
    dieu_blocks = _slice_by_matches(raw, dieu_matches)

    for d_start, d_end, d_m in dieu_blocks:
        dieu_block = raw[d_start:d_end].strip()
        d_num = d_m.group(1) if d_m else "?"
        
        # 2. TÃ¡ch theo Khoáº£n (1., 2., ...)
        khoan_num_matches = list(RX_KHOAN_NUM.finditer(dieu_block))
        use_word_khoan = False
        if not khoan_num_matches:
            khoan_word_matches = list(RX_KHOAN_WORD.finditer(dieu_block))
            if khoan_word_matches:
                use_word_khoan = True
                khoan_blocks = _slice_by_matches(dieu_block, khoan_word_matches)
            else:
                khoan_blocks = []
        else:
            khoan_blocks = _slice_by_matches(dieu_block, khoan_num_matches)

        # Náº¿u khÃ´ng cÃ³ khoáº£n, láº¥y cáº£ Ä‘iá»u
        if not khoan_blocks:
            title = f"[{law_code}] Äiá»u {d_num}"
            if len(dieu_block.split()) >= 8: # Lá»c rÃ¡c ngáº¯n
                passages.append(f"{title}\n\n{dieu_block}")
            continue

        # Náº¿u cÃ³ khoáº£n
        dieu_title_line = dieu_block.split('\n')[0] if '\n' in dieu_block else ""
        for k_start, k_end, k_m in khoan_blocks:
            khoan_block = dieu_block[k_start:k_end].strip()
            khoan_block = f"{dieu_title_line}\n{khoan_block}" # Giá»¯ context tÃªn Ä‘iá»u
            
            k_num = k_m.group(1)
            k_type = "Khoáº£n" if use_word_khoan else "Má»¥c"
            k_title = f"[{law_code}] Äiá»u {d_num} {k_type} {k_num}"
            
            if len(khoan_block.split()) >= 10:
                passages.append(f"{k_title}\n\n{khoan_block}")
                
    return passages

def update_qdrant(docs: List[str], collection_name="gtdb-1"):
    """Táº¡o embedding vÃ  Ä‘áº©y vÃ o Qdrant"""
    print("â³ Äang táº¡o Embedding cho Qdrant...")
    # DÃ¹ng model tiáº¿ng Viá»‡t nhÆ° trong notebook
    model = SentenceTransformer('AITeamVN/Vietnamese_Embedding')
    embeddings = model.encode(docs, show_progress_bar=True)

    client = QdrantClient(
        url=os.getenv("QDRANT_URL"),
        api_key=os.getenv("QDRANT_API_KEY"),
        prefer_grpc=False
    )

    # Äáº£m báº£o collection tá»“n táº¡i
    if not client.collection_exists(collection_name):
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.COSINE)
        )

    points = []
    for i, doc in enumerate(docs):
        # Táº¡o payload metadata Ä‘á»ƒ trÃ­ch dáº«n sau nÃ y
        # Regex Ä‘Æ¡n giáº£n Ä‘á»ƒ láº¥y law_id tá»« title "[36/2024/QH15]..."
        law_id = doc.split("]")[0].replace("[", "") if "]" in doc else "unknown"
        
        points.append(PointStruct(
            id=str(uuid.uuid4()),
            vector=embeddings[i].tolist(),
            payload={
                "text": doc,
                "law_id": law_id,
                "type": "law_chunk"
            }
        ))
    
    # Upsert batch
    client.upsert(collection_name=collection_name, points=points)
    print(f"âœ… ÄÃ£ Ä‘áº©y {len(points)} Ä‘oáº¡n vÄƒn báº£n vÃ o Qdrant.")

def update_hipporag(docs: List[str]):
    """Cáº­p nháº­t index cho HippoRAG"""
    print("â³ Äang Indexing HippoRAG (Viá»‡c nÃ y cÃ³ thá»ƒ tá»‘n thá»i gian vÃ  tiá»n OpenAI)...")
    # LÆ°u Ã½: HippoRAG hiá»‡n táº¡i thÆ°á»ng index batch lá»›n. 
    # Náº¿u append, cáº§n cáº©n trá»ng vá»›i save_dir cÅ©.
    
    hippo = HippoRAG(
        save_dir="outputs", # Folder chá»©a graph cÅ©
        llm_model_name="gpt-4o-mini",
        embedding_model_name="text-embedding-3-small"
    )
    
    # HÃ m index cá»§a HippoRAG sáº½ cháº¡y OpenIE, táº¡o graph vÃ  lÆ°u láº¡i
    hippo.index(docs=docs)
    print("âœ… ÄÃ£ cáº­p nháº­t HippoRAG.")

def ingest_new_file(file_path: str):
    print(f"ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½: {file_path}")
    
    # 1. Láº¥y tÃªn luáº­t lÃ m code (VD: 36-2024-QH15)
    filename = os.path.basename(file_path).replace(".pdf", "")
    
    # 2. Äá»c vÃ  Split
    raw_text = extract_text_from_pdf(file_path)
    if not raw_text: return
    
    chunks = split_passages(raw_text, law_code=filename)
    print(f"-> TÃ¡ch Ä‘Æ°á»£c {len(chunks)} Ä‘oáº¡n (passages).")
    
    # 3. Äáº©y Qdrant (Vector Search)
    update_qdrant(chunks)
    
    # 4. Äáº©y HippoRAG (Graph Search)
    update_hipporag(chunks)

if __name__ == "__main__":
    # Test vá»›i má»™t file cá»¥ thá»ƒ
    # Äáº£m báº£o báº¡n Ä‘Ã£ cÃ i poppler-utils: sudo apt install poppler-utils
    ingest_new_file("data/new_laws/ThongTu_24_2023.pdf")