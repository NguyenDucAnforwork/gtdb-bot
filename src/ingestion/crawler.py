# src/ingestion/crawler.py
"""
Web crawler cho thuvienphapluat.vn
Crawl vƒÉn b·∫£n ph√°p lu·∫≠t t·ª´ URL v√† tr√≠ch xu·∫•t n·ªôi dung
"""

import requests
from bs4 import BeautifulSoup
from typing import Dict, Optional
import re

class ThuvienPhaplaatCrawler:
    """Crawler cho thuvienphapluat.vn"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def crawl(self, url: str) -> Optional[Dict[str, str]]:
        """
        Crawl vƒÉn b·∫£n t·ª´ URL
        
        Args:
            url: URL vƒÉn b·∫£n tr√™n thuvienphapluat.vn
            
        Returns:
            Dict v·ªõi 'law_code', 'title', 'content' ho·∫∑c None n·∫øu l·ªói
        """
        try:
            print(f"üåê Crawling: {url}")
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract law code v√† title
            law_code = self._extract_law_code(soup, url)
            title = self._extract_title(soup)
            content = self._extract_content(soup)
            
            if not content:
                print("‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung vƒÉn b·∫£n")
                return None
            
            print(f"‚úÖ Crawled: {law_code} - {title}")
            print(f"   Content length: {len(content)} chars")
            
            return {
                'law_code': law_code,
                'title': title,
                'content': content,
                'url': url
            }
            
        except Exception as e:
            print(f"‚ùå Crawl failed: {e}")
            return None
    
    def _extract_law_code(self, soup: BeautifulSoup, url: str) -> str:
        """Tr√≠ch xu·∫•t m√£ vƒÉn b·∫£n (VD: 158/2024/Nƒê-CP)"""
        
        # Method 1: T·ª´ URL pattern
        # https://thuvienphapluat.vn/van-ban/.../Nghi-dinh-158-2024-ND-CP-...
        url_match = re.search(r'(Nghi-dinh|Luat|Thong-tu|Quyet-dinh)-(\d+-\d+-[A-Z-]+)', url)
        if url_match:
            doc_type = url_match.group(1).replace('-', ' ')
            code = url_match.group(2).replace('-', '/')
            
            # Map document type
            type_map = {
                'Nghi dinh': 'Nƒê-CP',
                'Luat': 'QH',
                'Thong tu': 'TT',
                'Quyet dinh': 'Qƒê'
            }
            
            # Clean code (158/2024/ND/CP -> 158/2024/Nƒê-CP)
            for vn, abbr in type_map.items():
                if code.endswith(abbr.replace('-', '/')):
                    code = code.replace(abbr.replace('-', '/'), abbr)
                    break
            
            return code
        
        # Method 2: T·ª´ breadcrumb ho·∫∑c title
        breadcrumb = soup.find('ol', class_='breadcrumb')
        if breadcrumb:
            links = breadcrumb.find_all('a')
            for link in links:
                text = link.get_text(strip=True)
                code_match = re.search(r'(\d+/\d+/[A-Z-]+)', text)
                if code_match:
                    return code_match.group(1)
        
        # Fallback: "unknown"
        return "unknown"
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ vƒÉn b·∫£n"""
        
        # Method 1: h1.document-title
        title_elem = soup.find('h1', class_='document-title')
        if title_elem:
            return title_elem.get_text(strip=True)
        
        # Method 2: .title-document
        title_elem = soup.find('div', class_='title-document')
        if title_elem:
            return title_elem.get_text(strip=True)
        
        # Method 3: <title> tag
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text(strip=True)
        
        return "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"
    
    def _extract_content(self, soup: BeautifulSoup) -> Optional[str]:
        """Tr√≠ch xu·∫•t n·ªôi dung vƒÉn b·∫£n"""
        
        # Method 1: div#content1 (thuvienphapluat.vn structure)
        content_div = soup.find('div', id='content1')
        if content_div:
            return self._clean_content(content_div.get_text())
        
        # Method 2: .document-content
        content_div = soup.find('div', class_='document-content')
        if content_div:
            return self._clean_content(content_div.get_text())
        
        # Method 3: article tag
        article = soup.find('article')
        if article:
            return self._clean_content(article.get_text())
        
        return None
    
    def _clean_content(self, text: str) -> str:
        """L√†m s·∫°ch n·ªôi dung (remove extra whitespace, etc.)"""
        
        # Remove multiple newlines
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # Remove multiple spaces
        text = re.sub(r' {2,}', ' ', text)
        
        # Remove leading/trailing whitespace
        text = text.strip()
        
        return text


def crawl_document(url: str) -> Optional[Dict[str, str]]:
    """
    Convenience function ƒë·ªÉ crawl document
    
    Args:
        url: URL c·ªßa vƒÉn b·∫£n
        
    Returns:
        Document dict ho·∫∑c None
    """
    crawler = ThuvienPhaplaatCrawler()
    return crawler.crawl(url)
