# src/retrieval/enhanced_retriever.py
from langchain.retrievers import MergerRetriever
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun
import re

from langchain_community.retrievers.tavily_search_api import TavilySearchAPIRetriever

from config import settings
from src.retrieval.web_search import get_web_search_tool
from langchain.schema import Document
from langchain_qdrant import QdrantVectorStore
from typing import List, Dict, Any, Set
from pydantic import Field
from langchain_core.retrievers import BaseRetriever
import qdrant_client
from time import time
from qdrant_client.models import Filter, FieldCondition, MatchValue, Range, IntegerIndexParams, IntegerIndexType, PayloadSchemaType

# Mapping vÄƒn báº£n cÃ³ lá»—i font encoding (giá»‘ng HippoRAG)
DOCUMENT_MAPPING = {
    "Ngh nh 168-2024-N-CP": "Nghá»‹ Ä‘á»‹nh 168/2024/NÄ-CP",
    "Ngh nh 03-2021-N-CP": "Nghá»‹ Ä‘á»‹nh 03/2021/NÄ-CP",
    "Ngh nh 100-2019-N-CP": "Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP",
    "Ngh nh 123-2021-N-CP": "Nghá»‹ Ä‘á»‹nh 123/2021/NÄ-CP",
    "Lu t 35-2024-QH15": "Luáº­t 35/2024/QH15",
    "Lu t 36-2024-QH15": "Luáº­t 36/2024/QH15",
}

def format_qdrant_citation(metadata: Dict[str, Any]) -> str:
    """
    Format citation tá»« Qdrant metadata
    
    Args:
        metadata: Document metadata vá»›i law_id, article_id, clause_id
    
    Returns:
        Formatted citation string
    """
    citation_parts = []
    
    law_id = metadata.get("law_id")
    if law_id:
        # Fix encoding náº¿u cáº§n
        law_id = DOCUMENT_MAPPING.get(law_id, law_id)
        citation_parts.append(law_id)
    
    article_id = metadata.get("article_id")
    if article_id:
        citation_parts.append(f"Äiá»u {article_id}")
    
    clause_id = metadata.get("clause_id")
    if clause_id:
        citation_parts.append(f"Khoáº£n {clause_id}")
    
    point_id = metadata.get("point_id")
    if point_id:
        citation_parts.append(f"Äiá»ƒm {point_id}")
    
    if citation_parts:
        return ", ".join(citation_parts)
    else:
        return "KhÃ´ng xÃ¡c Ä‘á»‹nh nguá»“n"

def get_qdrant_retriever(embeddings):
    client = qdrant_client.QdrantClient(
        url=settings.QDRANT_URL,
        api_key=settings.QDRANT_API_KEY,
        prefer_grpc=True
    )

    # Khá»Ÿi táº¡o vector store Qdrant (dense / hybrid / sparse tÃ¹y config)
    vector_store = QdrantVectorStore(
        client=client,
        collection_name=settings.QDRANT_COLLECTION_NAME,
        embedding=embeddings,
        content_payload_key="text",
        # metadata_payload_key="metadata",
        # náº¿u báº¡n muá»‘n há»—n há»£p, báº¡n cÃ³ thá»ƒ thÃªm sparse_embedding hoáº·c retrieval_mode
        # vÃ­ dá»¥: retrieval_mode="hybrid" hoáº·c RetrievalMode.HYBRID
    )

    # DÃ¹ng as_retriever() vá»›i search_kwargs Ä‘á»ƒ control parameters
    retriever = vector_store.as_retriever(
        search_kwargs={
            "k": settings.QDRANT_RETURN_DOCS,  # sá»‘ lÆ°á»£ng documents tráº£ vá»
            # KhÃ´ng set score_threshold Ä‘á»ƒ láº¥y táº¥t cáº£ káº¿t quáº£
        }
    )
    return retriever

class EnhancedQdrantRetriever(BaseRetriever):
    """Custom Qdrant retriever with metadata handling vÃ  recursive search"""
    
    # Declare Pydantic fields
    embeddings: Any = Field(description="Embedding model")
    collection_name: str = Field(description="Qdrant collection name")
    similarity_threshold: float = Field(default=0.7, description="Similarity threshold")
    client: Any = Field(default=None, description="Qdrant client")
    
    def __init__(self, embeddings, similarity_threshold=0.7, **kwargs):
        # Initialize with proper Pydantic field assignment
        super().__init__(
            embeddings=embeddings,
            collection_name=settings.QDRANT_COLLECTION_NAME,
            similarity_threshold=similarity_threshold,
            client=qdrant_client.QdrantClient(
                url=settings.QDRANT_URL,
                api_key=settings.QDRANT_API_KEY,
                prefer_grpc=True
            ),
            **kwargs
        )
        print("âœ… EnhancedQdrantRetriever initialized")
    
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """Required method from BaseRetriever"""
        return self._search_documents(query)
    
    def _search_documents(self, query: str) -> List[Document]:
        """Main retrieval method with proper metadata extraction"""
        try:
            print(f"ğŸ” EnhancedQdrantRetriever searching for: {query}")
            
            # Generate query embedding
            query_embedding = self.embeddings.embed_query(query)
            
            # Search trong Qdrant
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=7,
                with_payload=True,
                score_threshold=self.similarity_threshold
            )
            
            documents = []
            
            for point in search_results:
                # ğŸ”§ Láº¥y content tá»« field "text"
                content = point.payload.get("text", "")
                
                if not content or not content.strip():
                    print(f"âš ï¸ Empty content for point {point.id}")
                    continue
                
                # ğŸ”§ Build metadata tá»« táº¥t cáº£ fields khÃ¡c (trá»« "text")
                metadata = {
                    "_id": point.id,
                    "_score": point.score,
                    "_collection": self.collection_name,
                }
                
                # ThÃªm táº¥t cáº£ fields khÃ¡c lÃ m metadata
                for key, value in point.payload.items():
                    if key != "text":  # KhÃ´ng include field content
                        metadata[key] = value
                
                # Táº¡o Document
                doc = Document(
                    page_content=content,
                    metadata=metadata
                )
                documents.append(doc)
            
            print(f"âœ… Found {len(documents)} documents with content")
            
            # Debug first document
            if documents:
                first_doc = documents[0]
                print(f"ğŸ“„ First doc preview:")
                print(f"   Content length: {len(first_doc.page_content)}")
                print(f"   Metadata keys: {list(first_doc.metadata.keys())}")
                print(f"   Content preview: {first_doc.page_content[:100]}...")
            
            return documents
            
        except Exception as e:
            print(f"âŒ EnhancedQdrantRetriever failed: {e}")
            import traceback
            traceback.print_exc()
            return []

class RecursiveQdrantRetriever(BaseRetriever):
    """Recursive retriever vá»›i metadata filtering"""
    
    # Declare Pydantic fields
    embeddings: Any = Field(description="Embedding model")  
    collection_name: str = Field(description="Qdrant collection name")
    similarity_threshold: float = Field(default=0.28, description="Similarity threshold")
    max_depth: int = Field(default=2, description="Maximum recursion depth")
    client: Any = Field(default=None, description="Qdrant client")
    
    def __init__(self, embeddings, max_depth=2, similarity_threshold=0.28, **kwargs):
        # Initialize with proper Pydantic field assignment
        super().__init__(
            embeddings=embeddings,
            collection_name=settings.QDRANT_COLLECTION_NAME,
            max_depth=max_depth,
            similarity_threshold=similarity_threshold,
            client=qdrant_client.QdrantClient(
                url=settings.QDRANT_URL,
                api_key=settings.QDRANT_API_KEY,
                prefer_grpc=True
            ),
            **kwargs
        )

        # --- táº¡o index cho cÃ¡c trÆ°á»ng metadata cáº§n filter ---
        try:
            # law_id lÃ  string â†’ dÃ¹ng keyword
            self.client.create_payload_index(
                collection_name=self.collection_name,
                field_name="law_id",
                field_schema=PayloadSchemaType.KEYWORD
            )
            # article_id lÃ  int â†’ dÃ¹ng IntegerIndexParams vá»›i lookup & range = True
            self.client.create_payload_index(
                collection_name=self.collection_name,
                field_name="article_id",
                field_schema=IntegerIndexParams(
                    type=IntegerIndexType.INTEGER,
                    lookup=True,
                    range=True
                )
            )
            print(f"âœ… Payload indexes ensured for fields law_id + article_id")
        except Exception as e:
            print(f"âš ï¸ Could not create payload indexes (maybe already exist): {e}")
            
        print(f"âœ… RecursiveQdrantRetriever initialized (max_depth={max_depth})")
    
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """Required method from BaseRetriever"""
        return self._recursive_search(query)
    
    def _recursive_search(self, query: str) -> List[Document]:
        """Main method vá»›i recursive search"""
        # 1. Base search
        base_docs = self._base_search(query)
        if not base_docs:
            return []
            
        # 2. Recursive search: báº¯t Ä‘áº§u vá»›i base_docs vÃ  depth=0
        seen_ids = {doc.metadata.get("_id") for doc in base_docs if doc.metadata.get("_id")}
        all_docs = base_docs.copy()
        
        # Báº¯t Ä‘áº§u Ä‘á»‡ quy
        self._search_related_recursively(base_docs, current_depth=0, seen_ids=seen_ids, all_accumulated_docs=all_docs)

        print(f"ğŸ¯ Total unique documents after recursion: {len(all_docs)}")
        return all_docs
    
    def _base_search(self, query: str) -> List[Document]:
        """Base search method"""
        try:
            print(f"ğŸ” RecursiveQdrantRetriever base search for: {query}")
            print(f"   Using similarity_threshold: {self.similarity_threshold}")
            
            # Generate query embedding
            query_embedding = self.embeddings.embed_query(query)
            
            # Search trong Qdrant - KHÃ”NG dÃ¹ng score_threshold Ä‘á»ƒ láº¥y háº¿t káº¿t quáº£
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=15,
                with_payload=True,
                # Bá» score_threshold Ä‘á»ƒ láº¥y táº¥t cáº£ káº¿t quáº£ nhÆ° get_qdrant_retriever
                # score_threshold=self.similarity_threshold
            )
            print(f"âœ… Base search in _base_search function found {len(search_results)} documents")
            
            documents = []
            
            for point in search_results:
                # ğŸ”§ Láº¥y content tá»« field "text"
                content = point.payload.get("text", "")
                
                if not content or not content.strip():
                    continue
                
                # Filter by similarity threshold manually (sau khi láº¥y káº¿t quáº£)
                if point.score < self.similarity_threshold:
                    # print(f"âš ï¸ Point {point.id} filtered out (score {point.score:.4f} < threshold {self.similarity_threshold})")
                    continue
                
                # ğŸ”§ Build metadata tá»« táº¥t cáº£ fields khÃ¡c (trá»« "text")
                metadata = {
                    "_id": point.id,
                    "_score": point.score,
                    "_collection": self.collection_name,
                    "_depth": 0  # Base search depth
                }
                
                # ThÃªm táº¥t cáº£ fields khÃ¡c lÃ m metadata
                for key, value in point.payload.items():
                    if key != "text":  # KhÃ´ng include field content
                        metadata[key] = value
                
                # Táº¡o Document
                doc = Document(
                    page_content=content,
                    metadata=metadata
                )
                documents.append(doc)
            
            print(f"âœ… Returned {len(documents)} documents after filtering")
            return documents
            
        except Exception as e:
            print(f"âŒ Base search failed: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _search_related_by_metadata(self, base_docs: List[Document], depth: int) -> List[Document]:
        next_depth = depth + 1
        if next_depth > self.max_depth:
            return []

        related_docs = []
        for doc in base_docs:
            law_id = doc.metadata.get("law_id")
            article_id = doc.metadata.get("article_id")
            if law_id is None or article_id is None:
                continue

            # Build filter: law_id = string, article_id = int
            filt = Filter(
                must=[
                    FieldCondition(key="law_id", match=MatchValue(value=str(law_id))),
                    FieldCondition(key="article_id", match=MatchValue(value=int(article_id)))
                ]
            )
            try:
                q_emb = self.embeddings.embed_query(doc.page_content[:200])
                results = self.client.search(
                    collection_name=self.collection_name,
                    query_vector=q_emb,
                    limit=5,
                    with_payload=True,
                    query_filter=filt
                )
                print(f"ğŸ” Related search for law_id={law_id}, article_id={article_id} at depth {next_depth}, found {len(results)} results")
                for pt in results:
                    content = pt.payload.get("text", "").strip()
                    if not content:
                        continue
                    metadata = {
                        "_id": pt.id,
                        "_score": pt.score,
                        "_depth": next_depth
                    }
                    for k, v in pt.payload.items():
                        if k != "text":
                            metadata[k] = v
                    related_docs.append(Document(page_content=content, metadata=metadata))
            except Exception as e:
                print(f"âš ï¸ Related search failed for law_id={law_id}, article_id={article_id}: {e}")

        print(f"ğŸ”„ Depth {next_depth}: found {len(related_docs)} related docs")
        return related_docs
    
    def _search_related_recursively(self, docs_to_process: List[Document], current_depth: int, seen_ids: Set, all_accumulated_docs: List[Document]):
        """
        HÃ m Ä‘á»‡ quy tÃ¬m kiáº¿m cÃ¡c tÃ i liá»‡u liÃªn quan dá»±a trÃªn trÃ­ch xuáº¥t text.
        """
        next_depth = current_depth + 1
        if next_depth > self.max_depth:
            return

        new_docs = []
        
        for doc in docs_to_process:
            content = doc.page_content.lower()
            law_id = doc.metadata.get("law_id")
            article_id = doc.metadata.get("article_id")

            # Chá»‰ xá»­ lÃ½ náº¿u cÃ³ Ä‘á»§ law_id vÃ  article_id Ä‘á»ƒ tham chiáº¿u
            if not law_id or not article_id:
                continue

            # Regex Ä‘á»ƒ tÃ¬m pattern "khoáº£n X Äiá»u nÃ y"
            # \b lÃ  ranh giá»›i tá»«, \d+ lÃ  má»™t hoáº·c nhiá»u chá»¯ sá»‘
            # Pattern nÃ y cÃ³ thá»ƒ cáº§n tinh chá»‰nh tÃ¹y vÃ o dá»¯ liá»‡u thá»±c táº¿ cá»§a báº¡n
            matches = re.findall(r"khoáº£n\s+(\d+)\s+Ä‘iá»u\s+nÃ y", content)
            print(f"TÃ¬m tháº¥y khoáº£n tham chiáº¿u trong doc {doc.metadata.get('_id')}: {matches}")
            
            if not matches:
                continue

            # Deduplicate cÃ¡c clause_id tÃ¬m tháº¥y trong cÃ¹ng 1 doc
            referenced_clause_ids = set(matches)
            
            for clause_id_str in referenced_clause_ids:
                try:
                     # Giáº£ sá»­ clause_id trong DB lÃ  integer. Náº¿u lÃ  string thÃ¬ bá» int()
                    target_clause_id = int(clause_id_str)
                    
                    # TÃ¬m kiáº¿m chÃ­nh xÃ¡c trong Qdrant báº±ng Filter
                    # LÆ°u Ã½: Cáº§n Ä‘áº£m báº£o 'clause_id' Ä‘Ã£ Ä‘Æ°á»£c index trong Qdrant
                    filt = Filter(
                        must=[
                            FieldCondition(key="law_id", match=MatchValue(value=str(law_id))),
                            FieldCondition(key="article_id", match=MatchValue(value=int(article_id))),
                            FieldCondition(key="clause_id", match=MatchValue(value=target_clause_id))
                        ]
                    )
                    
                    # DÃ¹ng scroll Ä‘á»ƒ láº¥y táº¥t cáº£ chunk khá»›p (thÆ°á»ng má»™t khoáº£n cÃ³ thá»ƒ bá»‹ chia thÃ nh nhiá»u chunk)
                    # Limit cÃ³ thá»ƒ Ä‘iá»u chá»‰nh tÃ¹y Ä‘á»™ dÃ i trung bÃ¬nh cá»§a khoáº£n
                    points, _ = self.client.scroll(
                        collection_name=self.collection_name,
                        scroll_filter=filt,
                        limit=5, 
                        with_payload=True,
                        with_vectors=False # KhÃ´ng cáº§n vector
                    )
                    
                    for point in points:
                        if point.id in seen_ids:
                            continue
                            
                        seen_ids.add(point.id)
                        
                        # Táº¡o document má»›i
                        content = point.payload.get("text", "")
                        if content:
                            metadata = {"_id": point.id, "_depth": next_depth, "_source_type": "referenced"}
                            for k, v in point.payload.items():
                                if k != "text":
                                    metadata[k] = v
                                    
                            new_doc = Document(page_content=content, metadata=metadata)
                            new_docs.append(new_doc)
                            all_accumulated_docs.append(new_doc)
                            print(f"  -> Found referenced: Law {law_id}, Art {article_id}, Clause {target_clause_id} (from doc {doc.metadata.get('_id')})")

                except ValueError:
                    # TrÆ°á»ng há»£p clause_id khÃ´ng pháº£i lÃ  sá»‘ há»£p lá»‡
                    continue
                except Exception as e:
                    print(f"âš ï¸ Error searching for referenced clause {clause_id_str}: {e}")

        if new_docs:
            print(f"ğŸ”„ Depth {next_depth}: found {len(new_docs)} new related docs. Continuing recursion...")
            # Gá»i Ä‘á»‡ quy vá»›i cÃ¡c tÃ i liá»‡u má»›i tÃ¬m Ä‘Æ°á»£c
            self._search_related_recursively(new_docs, next_depth, seen_ids, all_accumulated_docs)
        else:
            print(f"â¹ï¸ Depth {next_depth}: No new related docs found. Stopping recursion branch.")

def create_enhanced_retriever(embeddings):
    """
    Creates a combined retriever that merges results from a vector store and web search.
    """
    # 1. Vector Store Retriever (simple retriever without score filtering)
    qdrant_retriever = get_qdrant_retriever(embeddings)
    
    # 2. Recursive Qdrant Retriever with lower threshold (0.4 instead of 0.7)
    recursive_qdrant_retriever = RecursiveQdrantRetriever(
        embeddings=embeddings,
        max_depth=2,
        similarity_threshold=0.4  # Lower threshold to get more results
    )

    # 3. Merge the retrievers
    tavily_retriever = TavilySearchAPIRetriever(api_key=settings.TAVILY_API_KEY, k=settings.TAVILY_MAX_RESULTS)
    print("âœ… Tavily Search Retriever Created.", type(tavily_retriever))
    lotr = MergerRetriever(retrievers=[recursive_qdrant_retriever, tavily_retriever])
    print("âœ… Enhanced Retriever Created.")
 
    return lotr

def create_vector_only_retriever(embeddings):
    """
    Táº¡o retriever chá»‰ dÃ¹ng Qdrant vector store (khÃ´ng web search)
    
    Returns:
        RecursiveQdrantRetriever vá»›i citations tá»« metadata
    """
    print("ğŸ” Creating Vector-Only Retriever (Qdrant)...")
    
    retriever = RecursiveQdrantRetriever(
        embeddings=embeddings,
        max_depth=2,
        similarity_threshold=0.28
    )
    
    print("âœ… Vector-Only Retriever Created")
    return retriever

def create_hipporag_only_retriever():
    """
    Táº¡o retriever chá»‰ dÃ¹ng HippoRAG knowledge graph
    
    Returns:
        HippoRAGRetriever
    """
    print("ğŸ§  Creating HippoRAG-Only Retriever...")
    
    from src.retrieval.hipporag_retriever import get_hipporag_retriever
    
    retriever = get_hipporag_retriever(max_docs_per_query=3)
    
    print("âœ… HippoRAG-Only Retriever Created")
    return retriever

def create_combined_retriever(embeddings):
    """
    Táº¡o retriever káº¿t há»£p HippoRAG + Qdrant
    
    Combine results tá»«:
    - HippoRAG: Knowledge graph vá»›i citations tá»« doc titles
    - Qdrant: Vector store vá»›i citations tá»« metadata
    
    Returns:
        MergerRetriever combining both
    """
    print("ğŸ”„ Creating Combined Retriever (HippoRAG + Qdrant)...")
    
    # Get individual retrievers
    vector_retriever = create_vector_only_retriever(embeddings)
    hipporag_retriever = create_hipporag_only_retriever()
    
    # Merge them
    combined = MergerRetriever(retrievers=[hipporag_retriever, vector_retriever])
    
    print("âœ… Combined Retriever Created (HippoRAG + Qdrant)")
    return combined
